\documentclass{article}
\usepackage{krctran}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm2e}

\begin{document}

\procname{Труды Карельского научного центра РАН\\ \No 0. 2010. С.~1--4}
\udk{УДК 004.01:006.72 (470.22)}

\rustitle{Обзор методов и алгоритмов \newline по решению задачи разрешения лексической многозначности}
\engtitle{Word-sense disambiguation methods and algorithms review}

\rusauthor{С.~С.~Ткач, Е.~А.~Ярышкина, А.~Н.~Кириллов, И.~А.~Сихонина, Н.~И.~Коржицкий, А.~М.~Спирикова, А.~Л.~Чухарев, Т.~В.~Каушинис, А.~А.~Крижановский, Д.~С.~Шорец, Д.~Ю.~Янкевич}
\engauthor{S.~S.~Tkach, E.~A.~Yaryshkina, A.~N.~Kirillov, I.~A.~Sikhonina, N.~I.~Korzhitsky, A.~M.~Spirikova, A.~L.~Chuharev, T.~V.~Kaushinis, A.~A.~Krizhanovsky, D.~S.~Shorets, D.~Y.~Yankevich}


\organization{Институт прикладных математических исследований Карельского научного центра РАН}

\rusabstract{В статье дается краткий обзор методов и алгоритмов по решению задачи разрешения лексической многозначности.}
\engabstract{This paper gives a brief overview of word-sense disambiguation methods and algorithms.}

\ruskeywords{труды, шаблон, подготовка статьи.}
\engkeywords{transactions, template, article.}

\maketitle

\begin{articletext}
\section{Введение}
В статье представлен обзор методов и алгоритмов разрешения лексической многозначности (word-sense disambiguation или WSD). Верный выбор одного из значений многозначного слова для данного контекста, фразы, предложения и является результатом решения WSD-задачи.

Именно многозначность слов, их неоднозначность и зависимость значений слов от контекста являются причиной возникновения такой задачи и одновременно обуславливает сложность её решения. Уверенное решение WSD-задачи необходимо для точного машинного перевода и, на наш взгляд, является предтечей искусственного интеллекта.

Для этой задачи известно большое количество алгоритмов и методов решения, которые можно разделить на три группы: (1) WSD-методы с учителем; (2) WSD-методы без учителя и (3) WSD-методы на основе знаний. Далее в этом порядке и будут представлены методы и алгоритмы разрешения лексической многозначности.


\bfullwidth
\begin{center}
\section{WSD-методы с учителем}
\end{center}
\efullwidth

\section{Разрешение лексической многозначности методом ансамбля байесовских классификаторов}

\begin{flushright}
\textit{А. Л. Чухарев, Т. В. Каушинис}
\end{flushright}

В статье \cite{Pedersen 2000} рассматривается подход к разрешению лексической многозначности слов (WSD), подразумевающий создание ансамбля наивных байесовских классификаторов, каждый из которых основан на оценке вероятности вхождения определенных слов в контекст целевого слова, значение которого определяется.
\parindent=0,5cm

При разрешении лексической многозначности, представленной как задача обучения с учителем, применяют статистические методы и методы машинного обучения к размеченному корпусу. В таких методах словам корпуса, для которых указано значение, соответствует набор лингвистических свойств. Алгоритм обучения строит модель классификатора значений по этим лингвистическим особенностям.

Автор статьи \cite{Pedersen 2000} предлагает подход, основанный на объединении ряда простых классификаторов в ансамбль, который разрешает многозначность с помощью голосования простым большинством голосов. Педерсен утверждает \cite{Pedersen 2000}, что, во-первых, рассмотрение более сложного набора языковых особенностей или более сложных алгоритмов обычно не улучшает точность разрешения по сравнению с простыми языковыми особенностями, используемых в алгоритме обучения с учителем. Во-вторых, совместная встречаемость слов и словосочетаний имеет большее влияние на точность разрешения, оперирование такой лингвистической информацией как: часть речи или наличие отношений действие-объект.
\parindent=0,5cm


В рассматриваемой статье \cite{Pedersen 2000} в ансамбль объединяются наивные байесовские классификаторы. При таком подходе предполагается, что все переменные, участвующие в представлении проблемы, – условно независимы, при фиксированном значении переменной классификации. В проблеме разрешения лексической многозначности существует понятие контекста, в котором встречается многозначное слово. Он представляется в виде функции переменных $(F_1, F_2, \ldots , F_n)$, а значение многозначного слова представлено в виде классификационной переменной (S). Все переменные бинарные. Переменная, соответствующая слову из контекста, принимает значение ИСТИНА, если это слово находится на расстоянии определенного количества слов слева или справа от целевого слова. Совместная вероятность наблюдения определённой комбинации переменных контекста с конкретным значением слова выражается следующим образом: 
\\
\\
$p(F_1, F_2, \ldots , F_nS) = (p(S)\Pi^n_{i=1}p(F_i \vee S))$ 
\\
\\
где $p(S)$ и $p(F_i|S)$ --- параметры данной модели. Для оценки параметров достаточно знать частоты событий, описываемых взаимозависимыми переменными $(F_i,S)$. Эти значения соответствуют числу предложений, где слово, представляемое $F_i$, встречается в некотором контексте многозначного слова, упомянутого в значении $S$. Если возникают нулевые значения параметров, то они сглаживаются путём присвоения им по умолчанию очень маленького значения. После оценки всех параметров модель считается  обученной и может быть использована в качестве классификатора.
\parindent=0,5cm

Контекст в \cite{Pedersen 2000} представлен в виде bag-of-words (модель «мешка слов»). В этой модели выполняется следующая предобработка текста: удаляются знаки препинания, все слова переводятся в нижний регистр, все слова приводятся к их начальной форме (лемматизация). 
В \cite{Pedersen 2000} контексты делятся на два окна: левое и правое. В первое попадают слова, встречающиеся слева от неоднозначного слова, и, соответственно, во второе --- встречающиеся справа.
\parindent=0,5cm

Окна контекстов могут принимать 9 различных размеров: 0, 1, 2, 3, 4, 5, 10, 25 и 50 слов. Первым шагом в ансамблевом подходе является обучение отдельных наивных байесовских классификаторов для каждого из 81 возможных сочетаний левого и правого размеров окон. В статье  \cite{Pedersen 2000} наивный байесовский классификатор $(l,r)$ включает в себя $l$ слов слева от неоднозначного слова и $r$ слов справа. Исключением является классификатор (0,0), который не включает в себя слов ни слева, ни справа. В случае нулевого контекста классификатору присваивается \textbf{априорная вероятность} многозначного слова (равная  вероятности  встретить наиболее употребимое значение).
\parindent=0,5cm

Следующий шаг в \cite{Pedersen 2000} при построении ансамбля --- это выбор классификаторов, которые станут членами ансамбля. 81 классификатор группируется в три общие категории, по размеру окна контекста. Используются три таких диапазона: узкий (окна шириной в 0, 1 и 2 слова), средний (3, 4, 5 слов), широкий (10, 25, 50 слов). Всего есть 9 возможных комбинаций, поскольку левое и правое окна отделены друг от друга. Например, наивный байес (1,3) относится к диапазону категории (узкий, средний) поскольку он основан на окне из одного слова слева и окне из трёх слов справа.  Наиболее точный классификатор в каждой из 9 категорий диапазонов выбирается для включения в ансамбль. Затем каждый из 9 членов классификаторов голосует за наиболее вероятное значение слова с учетом контекста. После этого ансамбль разрешает многозначность путем присвоения целевому слову значения, получившего наибольшее число голосов. 
\parindent=0,5cm

\textbf{Экспериментальные данные.} Для экспериментов были выбраны английские слова \textit{line} и \textit{interest.} Источником статистических данных по этим словам послужили работы \cite{Leacock 1993}, \cite{Bruce 1994}. В статье приводится информация о частоте использования шести значений для каждого из этих слов (Табл. ~\ref{tbl3}, Табл. ~\ref{tbl4}). 

\begin{table}[H]
\centering
\caption{Число употреблений слова \textit{line} (столбец \textit{count}) для шести наиболее часто встречаемых значений (значения из тезауруса WordNet, столбец \textit{sense}) по данным корпусов \textit{ACL/DCI Wall Street Journal и American Printing House for the Blind}}
\begin{tabular}{|c|c|}
\hline
sense & count\\
\hline
product & 2218\\
written or spoken text & 405\\
telephone connection & 429\\
formation of people or things; queue & 349\\
an artificial division; boundary & 376\\
a thin, flexible object; cord & 371\\
\hline
total & 4148\\
\hline
\end{tabular}
\label{tbl3}
\end{table}

\begin{table}[H]
\centering
\caption{Число употреблений слова \textit{interest} (столбец \textit{count}) для шести наиболее часто встречаемых значений (значения из словаря Longman Dictionary of Contemporary English, столбец \textit{sense}). Этот набор данных был получен в 1994 году Брюсом и Виебе \cite{Bruce 1994} путём указания значений для всех вхождений слова \textit{interest} в корпус ACL/DCI Wall Street Journal}
\begin{tabular}{|c|c|}
\hline
sense & count\\
\hline
money paid for the use of money & 1252\\
a share in a company or business & 500\\
readiness to give attention & 361\\
advantage, advancement or favor & 178\\
activity that one gives attention to & 66\\
causing attention to be given to & 11\\
\hline
total & 2368\\
\hline
\end{tabular}
\label{tbl4}
\end{table}


\parindent=0,5cm
\textbf{Результаты экспериментов.} Итогом проделанной работы стали обучение и проверка 81 наивного байесовского классификатора на многозначных словах \textit{line} и \textit{interest.} Точность разрешения лексической многозначности составила 89\% для слова \textit{interest} и 88\% для слова \textit{line.} В \cite{Pedersen 2000} было получено, что ансамбль классификаторов с голосованием простым большинством даёт более высокую точность, чем взвешенное голосование. Например, для слова \textit{interest} при голосовании простым большинством точность составила 89\%, а взвешенное голосование дало только 83\%.




\section{WSD на основе нейронных сетей, построенных по данным машиночитаемых словарей}

\begin{flushright}
\textit{А. Н. Кириллов}
\end{flushright}

\parindent=0,5cm

Настоящий текст является рефератом статьи \cite{VERONIS 
1990}, в которой описан метод автоматического построения очень больших нейронных сетей (VLNN) с помощью текстов, извлекаемых из машинно-читаемых словарей (MRD), и рассмотрено использование этих сетей в задачах разрешения лексической неоднозначности (WSD). 

\parindent=0,5cm

В дальнейшем будем называть слова, смысл которых требуется установить целевыми словами. 

\parindent=0,5cm

Широко известен метод Леска \cite{LESK 1986}  использования  информации  из MRD для  задачи  WSD.  Суть этого метода состоит в вычислении  так называемой <<степени пересечения>>, т.е.  количества общих слов в словарных определениях слов из контекста (<<окна>>) условного размера, содержащего целевое слово. Основной недостаток метода Леска --- зависимость  от  словарной  статьи, т.е. от слов, входящих в нее. Стратегия преодоления  этого  недостатка --- использование словарных статей, определяющих слова, входящие в другие словарные статьи, начиная со словарных статей, соответствующих  словам  из  контекста. Таким образом, образуются  достаточно  длинные   пути  из слов, входящих в словарные статьи. Эта  идея  лежит в основе топологии (строения) VLNN.

\parindent=0,5cm

Использование нейронных сетей  для WSD  было предложено в работах \cite{COTTRELL 1983,WALTZ 1985}.   В рассматриваемой  статье для построения  VLNN использован  словарь  Collins  English  Dictionary.

\parindent=0,5cm

\textbf{Топология сети.} Целевое слово представлено  узлом, соединенным активирующими  связями со смысловыми узлами, представляющими все возможные смыслы слова, имеющиеся  в словарных статьях. Каждый смысловой узел, в свою очередь, соединен  активирующими  связями с узлами, представляющими  слова в словарной статье, соответствующей определению данного смысла. Процесс соединения повторяется многократно, создавая большую сеть взаимосвязанных узлов. В идеале сеть может содержать весь словарь. Авторы, по практическим соображениям, ограничиваются несколькими тысячами узлов и 10 --- 20 тысячами соединений. Слова представлены своими леммами (каноническими формами). Узлы, представляющие различные смыслы данного слова, соединены запрещающими (подавляющими) связями.

\parindent=0,5cm

\textbf{Алгоритм функционирования сети.} При запуске сети первыми активируются узлы входного  слова, которое кодируется согласно принятому правилу. Затем  каждый  входной узел посылает активирующий сигнал своим смысловым узлам, с которыми он соединен. В результате  сигналы распространяются по всей сети в течение определенного числа циклов. В каждом цикле узлы слова и  его смыслов получают обратные сигналы от узлов, соединенных с ними. Узлы конкурирующих смыслов посылают взаимно подавляющие сигналы.  Взаимодействие  сигналов  обратной  связи  и  подавления,  в соответствии со стратегией <<победитель получает все>>, позволяет увеличить активацию узлов-слов и соответствующих им правильных узлов-смыслов,  одновременно  уменьшая  активацию  узлов  соответствующих неправильным смыслам. После нескольких десятков  циклов сеть стабилизируется  в  состоянии, в котором активированы  только узлы-смыслы с наиболее активированными связями  с    узлами-словами. В статье  не  указан  алгоритм  настройки, т.е.  обучения  сети.  Видимо,  используется  метод  встречного  распространения (back  propagation).

\section{Сравнительные эксперименты в WSD: роль предпочтений в машинном обучении}

\begin{flushright}
\textit{Н. И. Коржицкий}
\end{flushright}

В работе Рэймонда Муни \cite{Mooney 1996} представлено одно из первых сравнений разных по природе методов WSD на одних и тех же данных. В статье \cite{Mooney 1996} проведена серия экспериментов, в которых сравнивалась способность различных обучающихся алгоритмов определять значение слова в зависимости от контекста. Эксперимент состоял в определении одного из шести значений слова \textit{line} в различных контекста. Данные для проведения экспериментов взяты из работы \cite{Leacock 1993}.
\parindent=0,5cm

В машинном обучении под термином \textit{bias} (пристрастие, тенденция, предпочтение)  понимается любое основание для выбора одного обобщения другому, вместо строгого соответствия примерам \cite{Mooney 1996}. В деревьях принятия решений предпочтение (\textit{bias}) отдаётся простым деревьям решений, в нейронных сетях --- линейным пороговым функциям, а в байесовском классификаторе --- функциям, учитывающим условную независимость свойств. Чем лучше <<предпочтение>> обучающегося алгоритма соответствует характеристикам конкретной задачи, тем лучше будет результат. Большинство обучающихся алгоритмов обладают <<предпочтением>> наподобие Бритвы Оккама, в таких алгоритмах выбираются гипотезы, которые могут быть представлены меньшим количеством информации на каком-нибудь языке представлений. Однако компактность, с которой (деревья решений, дизъюнктивная нормальная форма, сети с линейным пороговым значением) представляют конкретные функции --- может существенно различаться. Поэтому различные <<предпочтительные>> оценки могут работать лучше или хуже в конкретных задачах. Одной из основных целей в машинном обучении является поиск <<предпочтений>> с целью решения прикладных практических задач.
\parindent=0,5cm

Выбор правильного <<предпочтения>> и обучающегося алгоритма является сложной задачей. Простым подходом является автоматизация выбора метода при помощи внутренней перекрестной валидации. Другой подход meta-learning заключается в том, чтобы обучиться набору правил (или другому классификатору), предсказывающему, когда обучающийся алгоритм будет срабатывать наилучшим образом на примере с набором свойств присущих проблеме.
\parindent=0,5cm

Описанный в \cite{Mooney 1996} эксперимент заключается в определении значения слова \textit{line} (англ. линия) среди 6 возможных вариантов (строка, ряд, дивизия, телефон, веревка, продуктовая линия).  
\parindent=0,5cm

Для получения обучающей выборки брались предложения со словом \textit{line}, и им в соответствие ставилось одно из 6 значений. Распределение значений неравномерно: включение в список иточников журнала The Wall Street Journal привело к тому, что одно из значений встречалось в 5 раз чаще всех остальных \cite{Leacock 1993}.
\parindent=0,5cm

\bfullwidth
\begin{table}[H]
\centering
\caption{Шесть значений слова \textit{line} из Английского Викисловаря и Русского Викисловаря}
\begin{tabular}{|m{2cm}|m{2cm}|m{6cm}|m{4cm}|}

\hline
\textbf{ключевое слово} & \textbf{перевод} & \textbf{толкование на английском (Английский Викисловарь)} & \textbf{толкование на русском (Русский Викисловарь)}\\
\hline
text & строка & A small amount of text & ряд слов, букв или иных знаков, написанных или напечатанных в одну линию \\
\hline
formation & ряд & A more-or-less straight sequence of people, objects, etc., either arranged as a queue or column and often waiting to be processed or dealt with, or arranged abreast of one another in a row (and contrasted with a column), as in a military formation & несколько объектов, расположенных в линию или следующих один за другим\\
\hline
division & дивизия & A formation, usually made up of two or three brigades & тактическое воинское соединение \\
\hline
phone & телефон & The wire connecting one telegraphic station with another, a telephone or internet cable between two points: a telephone or network connection & то же, что телефонный номер \\
\hline
cord & веревка & A rope, cord, string, or thread, of any thickness  & гибкое и длинное изделие, — чаще всего сплетённое или свитое из льняных (или пеньковых, полимерных и т. п.) волокон или прядей \\
\hline
product & продуктовая линия & The products or services sold by a business, or by extension, the business itself & совокупность однородной продукции единого назначения  \\
\hline
\end{tabular}
\label{tbl5}
\end{table}
\efullwidth
\parindent=0,5cm

В работе \cite{Charles 1993} было установлено, что наиболее эффективными при решении задачи WSD являются алгоритмы на основе дерева решений (decision tree). Данный класс методов обходил по точности и скорости работы класс нейронных сетей. Другие исследования \cite{Mooney 1995} показали, что класс методов индуктивного логического программирования (inductive logic programming) справляется с задачей разрешения лексической многозначности слова лучше алгоритмов на основе дерева решений. 
\parindent=0,5cm

В серии экспериментов в \cite{Mooney 1996} сравнивались следующие методы: байесовский классификатор, перцептрон, C4.5, метод k-ближайших соседей и модификации алгоритма FOIL: PFOIL-DLIST, PROIL-DNF, PFOIL-CNF. Все алгоритмы были реализованы на языке Common Lisp, за исключением C4.5, который был написан на языке C.
\parindent=0,5cm

После проведения сравнительных экспериментов, заключавшихся в обучении и определении значения слова \textit{line,} было выяснено, что байесовский классификатор и перцептрон работают точнее других рассмотренных методов. 
\parindent=0,5cm

Эксперименты проводились с разными размерами обучающей выборки для того, чтобы выяснить, какого рода зависимость имеет место между точностью определения значения и размером выборки. На Рис. ~\ref{kor1} отображена зависимость точности работы алгоритмов от размера выборки. При увеличении размера обучающей выборки сначала происходит резкий рост точности, последующий прирост точности становится незначительным.

\begin{figure}[H]
\includegraphics[keepaspectratio=true,
 width=0.9\columnwidth]{p1.png}
\caption{Рост точности решения WSD задачи для разных алгоритмов (определение значения слова \textit{line}) при увеличении размера обучающей выборки \cite{Mooney 1996}. Алгоритмы: PFOIL-DLIST, PROIL-DNF, PFOIL-CNF, C4.5, Naive Bayes --- наивный баейсовский классификатор; Perceptron --- персептрон; 3 Nearest Neighbor --- метод 3-х ближайших соседей}
\label{kor1}
\end{figure}

Эксперименты учитывали не только точность определения значения, но и требовательность алгоритма к ресурсам в процессе обучения и работы. На Рис. ~\ref{kor2} можно увидеть зависимость времени обучения от размера выборки. Самыми быстрообучаемыми оказались байесовский классификатор и перцептрон, а самыми медленными --- нормальные формы (рис. ~\ref{kor2}).

\begin{figure}[H]
\includegraphics[keepaspectratio=true,
 width=0.9\columnwidth]{p2.png}
\caption{Зависимость времени затраченного на обучение алгоритмов от размера обучающей выборки \cite{Mooney 1996}. Алгоритмы: PFOIL-DLIST, PROIL-DNF, PFOIL-CNF, C4.5, Naive Bayes --- наивный баейсовский классификатор; Perceptron --- персептрон; 3 Nearest Neighbor --- метод 3-х ближайших соседей}
\label{kor2}
\end{figure}

На Рис. ~\ref{kor3} представлена зависимость времени работы алгоритмов от размера обучающей выборки. Время работы алгоритмов дает другую картину: байесовский классификатор и перцептрон работают долго при максимальном размере обучающей выборки, в то время как остальные методы решают WSD задачу за постоянное время (рис. ~\ref{kor3}). 

\begin{figure}[H]
\includegraphics[keepaspectratio=true,
 width=0.9\columnwidth]{p3.png}
\caption{Зависимость времени работы алгоритмов от размера обучающей выборки при определении значения слова \textit{line} \cite{Mooney 1996}. Алгоритмы: PFOIL-DLIST, PROIL-DNF, PFOIL-CNF, C4.5, Naive Bayes --- наивный баейсовский классификатор; Perceptron --- персептрон; 3 Nearest Neighbor --- метод 3-х ближайших соседей}
\label{kor3}
\end{figure}

\bfullwidth
\begin{center}
\section{WSD-методы без учителя}
\end{center}
\efullwidth


\section{Различение значений слов на основе векторов свойств, расширенных словарными толкованиями}

\begin{flushright}
\textit{А. М. Спирикова} 
\end{flushright}

Амрута Пурандаре и Тед Педерсен в 2004 году разработали <<Алгоритм различения значений на основе контекстных векторов>> (\textit{Сontext vector sense discrimination}) \cite{Purandare 2004}. В этом алгоритме 1 берется набор примеров употреблений исследуемого слова, 2 выполняется кластеризация этих примеров так, чтобы близкие по значению или связанные каким-либо образом слова объединились в одну группу \cite{Purandare 2004}.
\parindent=0,5cm

\textit{Word sense discrimination} --- это задача группировки нескольких употреблений данного слова в кластеры, где каждому кластеру соответствует определенное значение целевого слова. Подходы к решению этой проблемы основываются на дистрибутивной гипотезе, которая говорит о том, что: лингвистические единицы, встречающиеся в схожих контекстах, имеют близкие значения. Следует различать понятия \textit{различение значений слов} и \textit{разрешение лексической многозначности}. При \textit{различении значений слов} нет никаких предопределённых значений слова, присоединенных к кластерам; здесь, скорее, слова, употребляющиеся в схожих контекстах, группируются в кластеры (значения).
\parindent=0,5cm

При решении задачи \textit{различения значений} используются контекстные вектора: если целевое слово встречается в тестовых данных, то контекст этого слова представляется в виде вектора контекста. \textit{Вектор контекста} --- это средний вектор по векторам свойств каждого из слов контекста. \textit{Вектор свойств} содержит информацию о совместной встречаемости данного слова с другими словами, этот вектор строится по данным корпуса текстов на этапе обучения.
\parindent=0,5cm

Метод различения значений Пурандаре и Педерсена \cite{Purandare 2004} предназначен для работы при недостаточном объёме текстовых данных, при этом вектор свойств расширяется данными, извлечёнными из толкований словарей.
\parindent=0,5cm
Этот метод группирует в кластеры близкие по значению употребления целевого слова.
\parindent=0,5cm

\textbf{Построение матрицы встречаемости слов.} Первоначально строится матрица совместной встречаемости слов по данным обучающего корпуса (здесь тексты Wall Street Journal и Британского национального корпуса).
\parindent=0,5cm

Вектор свойств (строка матрицы) содержит информацию о совместной встречаемости данного слова с другими. Было решено в \cite{Purandare 2004}, что слова <<встречаются>>, если они находятся в тексте на расстоянии не более пяти словопозиций (то есть между ними находится не более трех слов). 
\parindent=0,5cm

\textbf{Обработка матрицы.} После создания матрицы выполняется разделение тестовых данных, то есть группировка примеров употреблений (фраз) с целевым словом. Каждому слову в примере употребления  в тестовых данных соответствует вектор свойств из матрицы встречаемости. Средний вектор свойств по всем словам соответствует вектору контекста. Таким образом, набор тестовых данных, включающих употребление исследуемого слова, преобразуется в набор контекстных векторов, каждый из которых соответствует одному из употреблений целевого слова.
\parindent=0,5cm

Различение значений происходит путем кластеризации контекстных векторов с помощью разделяющего (partitional) или иерархического <<сверху вниз>> (agglomerative) алгоритма кластеризации \cite{Jain 1999}, \cite{Jain 1988}, \cite{Zhao 2002}.  Получающиеся кластеры составлены из употреблений близких по значению фраз, и каждый кластер соответствует отдельному значению целевого слова. 
\parindent=0,5cm

\textbf{Векторы свойств, расширенные текстами толкований из словаря.} Векторы свойств, полученные по небольшому корпусу текстов, имеют очень малую размерность (несколько сотен), что не позволяет полностью описать закономерности совместной встречаемости слов. Для решения этой проблемы векторы свойств слов расширяются содержательными словами (content words), извлеченными из словарных толкований разных значений данного слова. В Табл. ~\ref{tbl2} представлены примеры толкований и содержательные слова для восьми значений слова <<история>> из Русского Викисловаря.

\bfullwidth
\begin{table}[H]
\centering
\caption{Словарные толкования (и содержательные слова) по данным статьи <<история>> из Русского Викисловаря. Серым цветом и курсивом выделены те слова, которые уже были в векторе слов, чёрным – новые слова из толкований, которыми будет расширен вектор}
\begin{tabular}{|c|m{10cm}|m{5cm}|}

\hline
№ & Текст значения & Содержательные слова\\
\hline
1 & закономерное, последовательное развитие, изменение действительности & \textit{\textcolor{gray}{развитие,}} изменение\\
\hline
2 & наука, изучающая факты, тенденции и закономерности развития человеческого общества & \textit{\textcolor{gray}{наука,}} факт, тенденция, закономерность\\
\hline
3 & наука, изучающая ход развития, последовательные изменения какой-либо области природы или культуры & \textit{\textcolor{gray}{наука, развитие,}} изменение\\
\hline
4 & последовательный ход развития, изменения чего-либо, совокупность фактов о развитии какого-либо явления & \textit{\textcolor{gray}{развитие,}} изменение, факт\\
\hline
5 & отдалённое время с его событиями, происшествиями; прошлое & время, событие, происшествие\\
\hline
6 & эпическое повествование, рассказ & повествование, \textit{\textcolor{gray}{рассказ}}\\
\hline
7 & смешная или неожиданная ситуация, происшествие, случай & ситуация, случай, происшествие\\
\hline
8 & скандал, неприятность & скандал, неприятность\\
\hline
\end{tabular}
\label{tbl2}
\end{table}
\efullwidth

\parindent=0,5cm

Предположим, например, что вектор свойств (столбец в матрице встречаемости) для слова \textit{история} имеет непустые значения в строках, соответствующих словам: \textit{книга, мир, наука, образование, развитие, рассказ.}
\parindent=0,5cm 

В Русском Викисловаре различные значения слова \textit{история} (Табл. ~\ref{tbl2}) включают содержательные слова: \textit{время, закономерность, изменение, наука, неприятность, повествование, происшествие,} развитие, рассказ, \textit{ситуация, скандал, случай, событие, тенденция, факт.} Таким образом, вектор свойств, соответствующий слову <<история>>, будет расширен новыми (отсутствующими ранее) словами из словаря: \textit{время, закономерность, изменение, неприятность, повествование, происшествие, ситуация, скандал, случай, событие, тенденция, факт.}
\parindent=0,5cm

В итоге, вектор свойств будет включать слова: \textit{время, закономерность, изменение,} книга, мир, наука, \textit{неприятность,} образование, \textit{повествование, происшествие,} развитие, рассказ, \textit{ситуация, скандал, случай, тенденция, факт.}
\parindent=0,5cm

Для оценки результатов тестовым примерам употребления присваивали вручную теги значений. Кластеру присваивалось то значение, примеров употребления которого в нём было больше всего. 
\parindent=0,5cm

Авторами было проведено 75 экспериментов с использованием 72 слов из корпуса SENSEVAL-2 и со словами \textit{line, hard} и \textit{serve.}
\parindent=0,5cm

В тестовых данных SENSEVAL-2 примеры употреблений включали 2-3 предложения. Для каждого слова было дано от 50 до 200 примеров употреблений в тестовых и тренировочных данных. Для этих слов известно много (порядка 8-12) значений. Малое число примеров при большем числе значений привело к тому, что для некоторых значений оказалось мало примеров употреблений. 43 из 72 слов SENSEVAL-2 показали улучшение F-меры и полноты (recall) при расширении вектора свойств текстами толкований словаря. Однако для 29 слов F-мера стала хуже, что, возможно, говорит о трудностях и несовершенстве метода. Для окончательной оценки необходима большая экспериментальная база: не десятки слов, а десятки и сотни тысяч.
\parindent=0,5cm

Данный метод может быть полезен при различении  значений слов без учителя при небольшом количестве обучающих данных.

\section{Автоматический поиск и кластеризация похожих слов}

\begin{flushright}
\textit{Д. С. Шорец} 
\end{flushright}

В работе \cite{Dekang 1998} представлена методология автоматического создания тезауруса, основанная на анализе корпуса текста и вычислении сходства слов, близости их значений. Значение незнакомого слова часто можно определить по контексту \cite{Eugene 1975}.  Рассмотрим, например, следующий текст:
\begin{flushright}
(1)\textbf{\textit{Бутылка Tezg\"uino стоит на столе. Всем нравится Tezg\"uino. Tezg\"uino может привести к опьянению. Мы делаем Tezg\"uino из зерна.}} 
\end{flushright}

Из этого контекста можно предположить, что \textit{Tezg\"uino} --- это алкогольный напиток, приготовленный из зерна.

Задача поиска похожих слов \textit{(similar words)} является первым шагом в определении значения слова. Тогда при обработке корпуса, включающего предложение (1), результатом должно быть определение близости значения слова \textit{Tezg\"uino} к словам \textit{<<пиво>>, <<вино>>, <<водка>>.}

\textbf{Методология автоматического создания тезауруса.} Для вычисления сходства между словами в работе \cite{Dekang 1998} использован парсер \cite{Dekang 1993}, извлекающий тройки из текста. Тройки зависимостей от англ. \textit{dependency triple} (далее просто <<тройки>>) состоят из двух слов и грамматического отношения между ними. Символ $IIw, r, w'll$ означает частоту в корпусе тройки $(w, r, w')$, где $w, w'$ --- это слова в нормальной форме, $r$ --- синтаксическое отношение. Произвольное слово или отношение обозначается символом-джокером «*». Например, \textit{Ilcook, obj,} *|| означает число троек со словом \textit{cook} и отношением \textit{obj}. 

Для примера рассмотрим предложение \textit{<<У меня есть коричневая собака>>.} Из него извлечем следующие тройки:

\begin{center}
||коричневый, прил\_сущ, собака||\\
||есть, гл\_сущ, собака|| 
\end{center}

Определим следующие моменты:
\begin{enumerate} 
\item \textit{Описание слова w} --- это частоты всех троек ($w$, *, *) в корпусе,  то есть всех троек, включающих $w$. Описание слова $w$ является вектором.
\item \textit{<<Пересечение>> двух слов} --- это тройки, представленные в описании обоих слов; это пересечение векторов.
\end{enumerate}

Сходство между двумя объектами вычисляется как количество информации в <<пересечении>> двух объектов (2), деленное на количество информации в описании двух объектов (1), далее обозначено как функция \textit{\textbf{sim}} \cite{Dekang 1997}.

Предположив, что частоты троек не зависят друг от друга, получаем, что информация, представленная в описание слова $w$, равна сумме информации по каждой из уникальных троек в описании слова $w$. 

Для измерения информации в утверждении \textit{\textbf{IIw, r, w'll=с}} выполним следующее:

\begin{enumerate}
\item измерим количество информации в утверждении, что произвольная тройка, извлеченная из текста, будет наша тройка \textit{\textbf{(w, r, w')}} при условии, что значение \textit{\textbf{IIw, r, w'll}} --- не известно;
\item измерим то же при условии, что значение \textit{\textbf{IIw, r, w'll}} --- известно;
\item разница этих двух количеств является ответом.
\end{enumerate}

Вероятность встретить в тексте тройку \textit{\textbf{(w, r, w')}} можно рассматривать как одновременное возникновение трех событий:

\textbf{A:} случайно выбранное слово - это $w$;

\textbf{B:} случайно выбранное отношение- это $r$;

\textbf{C:} случайно выбранное слово - это $w'$;

\begin{enumerate}
\item Когда значение \textit{\textbf{IIw, r, w'll}} неизвестно, то предполагаем, что \textbf{А} и \textbf{С} являются условно независимыми при наличии события \textbf{В}.  Вероятность наступления сразу трех этих событий составляет \textbf{$P_{MLE}(B) P_{MLE} (A|B) P_{MLE} (C|B)$}, где \textbf{$P_{MLE}$} --- это оценка максимального правдоподобия распределения вероятностей (\textit{maximum likelihood estimation})

$P_{MLE}(B)=\frac{||*,r,*||}{||*,*,*||}$\\
$P_{MLE}(A|B)=\frac{||w,r,*||}{||*,r,*||}$\\
$P_{MLE}(C|B)=\frac{||*,r,w'||}{||*,r,*||}$

\item Когда значение \textit{\textbf{IIw, r, w’ll}} известно, можно сразу получить $P_{MLE}(A,B,C)$:

$P_{MLE}(A,B,C)=\frac{||w,r,w'||}{||*,*,*||}$
\item Пусть \textit{\textbf{I(w,r, w’)}} обозначает количество информации, содержащейся 
в утверждении \textit{\textbf{IIw, r, w'll=с}}. Можно вычислить это значение так:

\end{enumerate}

\bfullwidth
\begin{center}
$I(w,r,w’)=-log(P_{MLE}(B) P_{MLE} (A|B) P_{MLE} (C|B))-(-log(P_{MLE}(A,B,C))=log\frac{||w,r,w'||\times||*,r,*||}{||w,r,*||\times||*,r,w'||}$
\end{center}
\efullwidth

Отметим, что значение \textit{\textbf{I(w,r,w’)}} равно количеству взаимной информации (\textit{mutual information}) между $w$ и $w'$ \cite{Donald 1990}.

Пусть $T(w)$ --– это множество пар \textit{\textbf{(r, w')}}, при которых $log\frac{||w,r,w'||\times||*,r,*||}{||w,r,*||\times||*,r,w'||}$ имеет положительное значение. Определим значение сходства (похожести) двух слов \textit{\textbf{w1}} и \textit{\textbf{w2}} с помощью формулы:

\bfullwidth
\begin{center}
$sim(w_1,w_2)=\frac{\sum_{(r,w)\in T(w_1)\cap T(w_2)}(I(w_1,r,w)+I(w_2,r,w))}{\sum_{(r,w)\in T(w_1)}I(w_1,r,w)+\sum_{(r,w)\in T(w_2)}I(w_2,r,w)}$
\end{center}
\efullwidth

\textbf{Практическая реализация метода.} Был обработан корпус, включающий 64 млн. слов. Из него было извлечено 56,6 миллионов троек, включающих 8,7 миллиона уникальных троек.

Сам корпус был разбит на классы по частям речи.  Исследовалось попарно сходство между всеми глаголами, всеми существительными, всеми прилагательными/наречиями по формуле $sim(w1 , w2)$. Для каждого слова был построен аналог словарной статьи в тезаурусе, включающий упорядоченный набор 200 наиболее похожих слов. Статья в тезаурусе для слова w имела следующий формат:\\

$w(pos):w_1,s_1,w_2,s_2,\ldots,w_N,s_N$\\

где \textit{pos} --– это часть речи, $w_i$ ---  это похожее слово, $s_i$ --- это значение сходства $w$ и $w_i$, слова упорядочены по убыванию значения сходства.

Два слова являются \textit{парой взаимных ближайших соседей (RNN от respective nearest neighbors)}, если они являются наиболее похожими словами друг для друга (первыми в списке из двухсот слов). С помощью программы удалось получить 543 пары RNN существительных, 212 пар RNN глаголов, 382 пары RNN прилагательных/наречий в созданном автоматически тезаурусе. В Табл.~\ref{tabshor} представлен список каждого 10-го RNN для глаголов.

\bfullwidth
\begin{table}[H]
\centering
\caption{Список пар взаимных ближайших соседей (RNN) глаголов}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Ранг} & \textbf{RNN} & \textbf{Значение сходства}\\
\hline
1 & \textit{fall rise} & 0,67\\
\hline
11 & \textit{injure kill} & 0,38\\
\hline
21 & \textit{concern worry} & 0,34\\
\hline
31 & \textit{convict sentence} & 0,29\\
\hline
41 & \textit{limit restrict} & 0,27\\
\hline
51 & \textit{narrow widen} & 0,26\\
\hline
61 & \textit{attract draw} & 0,24\\
\hline
71 & \textit{discourage encourage} & 0,23\\
\hline
81 & \textit{hit strike} & 0,22\\
\hline
91 & \textit{distregard ignore} & 0,21\\
\hline
101 & \textit{overstate understate} & 0,20\\
\hline
111 & \textit{affirm reaffirm} & 0,18\\
\hline
121 & \textit{inform notify} & 0,17\\
\hline
131 & \textit{differ vary} & 0,16\\
\hline
141 & \textit{scream yell} & 0,15\\
\hline
151 & \textit{laugh smile} & 0,143\\
\hline
161 & \textit{compete cope} & 0,136\\
\hline
171 & \textit{add whisk} & 0,130\\
\hline
181 & \textit{blossom mature} & 0,12\\
\hline
191 & \textit{smell taste} & 0,11\\
\hline
201 & \textit{bark howl} & 0,10\\
\hline
211 & \textit{black white} & 0,07\\
\hline
\end{tabular}
\label{tabshor}
\end{table}
\efullwidth

\section{Разрешение многозначности в биомедицинских текстах с помощью методов кластеризации без учителя}

\begin{flushright}
\textit{Е. А. Ярышкина} 
\end{flushright}

В статье \cite{Savova 2005} изучаются уже существующие методы кластеризации без учителя и их эффективность для решения лексической многозначности при обработке текстов по биомедицине. Решение проблем лексической многозначности в данной области включает в себя не только традиционные задачи присвоения ранее определенных смысловых значений для терминов, но так же и обнаружения новых значений для них, ещё не включённых в данную онтологию.
\parindent=0,5cm

Авторы описали методологию способа разрешения лексической многозначности без учителя, учитываемые лексические признаки и наборы экспериментальных данных. В качестве оценки эффективности алгоритмов кластеризации текста была предложена F-мера. 
\parindent=0,5cm

Подход для решения поставленной задачи --- это разделение контекстов (фрагментов текста), содержащих определенное целевое слово на кластеры, где каждый кластер представляет собой различные значения целевого слова. Каждый кластер состоит из близких по значению контекстов. Задача решается в предположении, что используемое целевое слово в аналогичном контексте будет иметь один и тот же или очень похожий смысл. 
\parindent=0,5cm

Процесс кластеризации продолжается до тех пор, пока не будет найдено предварительно заданное число кластеров. В данной статье выбор шести кластеров основан на том факте, что это больше, чем максимальное число возможных значений любого английского слова, наблюдаемое среди данных (большинство слов имеют два-три значения). Нормализация текста не выполняется.
\parindent=0,5cm

Данные в этом исследовании состоят из ряда контекстов, которые включают данное целевое слово, где у каждого целевого слова вручную отмечено --- какое значение из словаря было использовано в этом контексте. Контекст --- это единственный источник информации о целевом слове. Цель исследования --- преобразовать контекст в контекстные вектора первого и второго порядка \cite{epr:website}. Контекстные вектора содержат следующие «лексические свойства»: биграммы, совместную встречаемость и совместную встречаемость целевого слова. Биграммами являются как двухсловные словосочетания, так и любые два слова, расположенные рядом в некотором тексте. Для лингвистических исследований могут быть полезны только упорядоченные наборы биграмм \cite{Averin 2006}. 
\parindent=0,5cm

Экспериментальные данные --- это набор NLM WSD \cite{UMLS:website} (NLM --- национальная библиотека медицины США), в котором значения слов взяты из UMLS (единая система медицинской терминологии). UMLS имеет три базы знаний: 
\begin{itemize}
\item Метатезаурус включает все термины из контролируемых словарей (SNOMED-CT, ICD и другие) и понятия, которые представляют собой кластера из терминов, описывающих один и тот же смысл. 
\item Семантическая сеть распределяет понятия на 134 категории и показывает отношения между ними. SPECIALIST-лексикон содержит семантическую информацию для терминов Метатезауруса. 
\item Medline --- главная библиографическая база данных NLM, которая включает приблизительно 13 миллионов ссылок на журнальные статьи в области науки о жизни с уклоном в биомедицинскую область.
\end{itemize}
\parindent=0,5cm

Авторы успешно проверили по три конфигурации существующих методов (PB --- Pedersen and Bruce \cite{Pedersen 1997}, SC --- Schutze \cite{Schutze 1998}) и оценили эффективность использования SVD (сингулярное разложение матриц). Методы PB основаны на контекстных векторах первого порядка --- признаки одновременного присутствия целевого слова или биграммы. Рассчитывается среднее расстояние между кластерами или применяется  метод бисекций. PB методы подходят для работы с довольно большими наборами данных. Методы SC основаны на представлениях второго порядка --- матрицы признаков одновременного присутствия или биграммы, где каждая строка и столбец --- вектор признаков первого порядка данного слова. Так же рассчитывается среднее расстояние между кластерами или применяется  метод бисекций. SC методы подходят для обработки небольших наборов данных.  

\parindent=0,5cm
Метод SC2 (признаки одновременного присутствия второго порядка, среднее расстояние между элементами кластера в пространстве подобия) с применением и без SVD показал лучшие результаты: всего 56 сравниваемых экземпляров, в 47 случаях метод SC2 показал наилучшие результаты, в 7 случаях результаты незначительно отличаются от других проверяемых методов.
\parindent=0,5cm

Все эксперименты, указанные в исследовании, выполнялись с помощью пакета SenseClusters \cite{SC:website}. В ходе исследования было проведено два эксперимента для разных наборов данных. Маленький тренировочный набор --- это набор NLM WSD, который включает 5000 экземпляров для 50 часто встречаемых неоднозначных терминов из Метатезауруса UMLS. Каждый неоднозначный термин имеет по 100 экземпляров с указанным вручную значением. У 21 термина максимальное число экземпляров находится в пределах от 45 до 79 экземпляров. У 29 терминов число экземпляров от 80 до 100 для  конкретного значения. Стоит отметить, что каждый термин имеет категорию «ни одно из вышеупомянутых», которая охватывает все оставшиеся значения, не соответствующие доступным в UMLS. Большой тренировочный набор является реконструкцией «1999 Medline», который был разработан Weeber \cite{Weeber 2001}. Были определены все формы из набора NLM WSD и сопоставлены с тезисами «1999 Medline». Для создания тренировочного набора экземпляров использовались только те тезисы из «1999 Medline», которым было найдено соответствие в наборе NLM WSD.
\parindent=0,5cm

Использование целиком текста аннотации статьи в качестве контекста приводит к лучшим результатам, чем использование отдельных предложений. С одной стороны, большой объем контекста, представленный аннотацией, дает богатую коллекцию признаков, с другой стороны, в коллекции WSD представлено небольшое число контекстов.

\section{Выявление значений слов из текста}
\begin{flushright}
\textit{Д. Ю. Янкевич} 
\end{flushright}

В статье \cite{Pantel 2002} представлен алгоритм автоматического обнаружения значений слов в тексте, названный \textit{кластеризация посредством комитетов} (Clustering By Committee (CBC)). Также авторы предлагают методологию оценки для автоматического измерения точности и полноты найденных значений.
\parindent=0,5cm

\textbf{Алгоритм} первоначально находит множество небольших кластеров, называемых комитетами, каждый из которых представляет собой одно из значений определяемого слова. Центр тяжести  членов комитета (мера связности с определяемым словом) используется в качестве вектора признаков кластера.
\parindent=0,5cm

CBC состоит из трех этапов.
\parindent=0,5cm

На этапе I для каждого элемента (слова) вычисляются $k$ наиболее похожих слов. Сначала весь список относящихся к слову значений сортируется по убыванию значений связи согласно формуле точечной взаимной информации (pointwise mutual information (PMI) \cite{Manning 1999}), а затем, с помощью иерархического кластерного анализа по \textit{методу средней связи} \cite{Kim 1989}, вычисляется сходство между всеми элементами кластера попарно. Значение функции PMI \cite{Manning 1999} между предполагаемым значением слова (контекстом) и элементом (словом) вычисляется следующим образом: пусть x --- это рассматриваемый элемент, а y --- контекст. \textit{Точечная взаимная информация} между x и y определена как:

$pmi(x;y)\equiv log \frac{p(x,y)}{p(x)p(y)}=log\frac{p(x|y)}{p(x)}=log\frac{p(y|x)}{p(y)}$
\parindent=0,5cm

При кластеризации посредством метода средних связей \textit{(average-link clustering)} вычисляется среднее сходство между данным объектом и всеми объектами в кластере, а затем, если найденное \textit{среднее значение сходства} достигает или превосходит некоторый заданный пороговый уровень сходства, объект присоединяется к этому кластеру \cite{Kim 1989}. Сложность этого алгоритма $O(n^2 * log (n))$, где $n$ --- число кластеризуемых элементов \cite{Jain 1999}.
\parindent=0,5cm

В этапе I оценка (PMI) означает предпочтение большим и тесно связанным кластерам.
\parindent=0,5cm

На II этапе строится набор небольших кластеров, где элементы каждого кластера образуют комитет. В ходе работы <<Алгоритма 1>> формируется как можно больше комитетов при условии, что каждый вновь созданный комитет не слишком похож на любой из уже существующих комитетов. Если условие нарушается, комитет просто отбрасывается. Алгоритм описан ниже подробно.

\bfullwidth
\begin{algorithm}[H]
\SetAlgoLined
\KwData{Список элементов $E$, которые будут сгруппированы, база данных сходства $S$, из фазы I, пороги $\theta1$ и $\theta2$ (с помощью порога $\theta1$ сохраняются только те кластеры, которые имеют значения, отличные от ранее обнаруженных, порог $\theta2$ позволяет обнаружить элементы, не принадлежащие ни одному из кластеров)}
\KwResult{$C$ --- список комитетов}
\textbf{Step 1:}\\
\ForEach{$е \in E$}{\begin{enumerate}
\item Кластер k наиболее <<близких>> (похожих) элементов е из S с помощью метода  средней связи
\item Для каждого обнаруженного кластера $c$ вычислить следующую оценку:
$val = | C | * avgsim(c)$, где $| C |$ --- количество элементов $c$ и $avgsim(c)$ --- усреднённое сходство между всеми парами элементов кластера $c$.
\item Записать кластер с наивысшей оценкой в список $L$\end{enumerate}}
\textbf{Step 2:}\\
Сортировка кластеров в списке $L$ в порядке убывания их оценок $val$\\
\textbf{Step 3:}\\
$C = \emptyset$ // Пусть $С$ перечень комитетов, изначально пустой\\
\ForEach{$c \in L$}{// в отсортированном по убыванию порядке:
\begin{enumerate}
\item Вычислить центр тяжести, усредняя поэлементно значение векторов, и вычислить вектор  PMI центроида (точно так же, как мы делали для отдельных элементов)
\item Если схожесть $c$ и центроида каждого комитета, ранее добавленного к $C$, ниже порогового $\theta1$, то следует добавить $c$ в $C$
\end{enumerate}}
\textbf{Step 4:}\\
\If{$C=\emptyset$}{\Return $C$}
$R = \emptyset$ // $R$ --- это множество остатков, то есть элементов, не охваченных ни одним из кластеров\\
\ForEach{$e \in E$}{\ForEach{$c \in C$}{\If{$sim(e, c) < \theta2$ // $sim(e,c)$ --- схожесть $e$ каждому комитету из $C$}{$R += e$ // то следует добавить $e$ в список остатков $R$}}}
\If{$R=\emptyset$}{\Return $C$ \Else{\Return $C \cup Algorithm1(R, S, \theta1, \theta2)$}}
\caption{II фаза кластеризации посредством комитетов (CBC)}
\label{alg}
\end{algorithm}
\efullwidth

В результате второго этапа построения CBC остаются кластеры, связанные более тесно (имеющие большее значение val, см. step 1 и 3 алгоритма ~\ref{alg}).
\parindent=0,5cm

На заключительном III этапе работы алгоритма каждый элемент e присваивается наиболее подходящему кластеру следующим образом (при этом центроид членов комитета используется в качестве вектора характеристик кластера):

\bfullwidth
\begin{algorithm}[H]
\SetAlgoLined

\KwResult{Итоговые кластеры с максимальным значением связи (val) между словами}
Пусть $С$ список кластеров (изначально пустых)\\
Пусть $S$ это топ-200 кластеров, схожих с $e$\\
\While{$S$ не пуст}{пусть $c \in S$ наиболее близкий кластер к $e$\\
\If{сходство $(e, c) < \sigma$}{\textbf{конец цикла}}
\If{$c$ не схож ни с одним кластером в $C$}{присвоить $e$ к $c$\\
удалить из $e$ его характеристики, которые перекрываются с характеристиками $c$}
удалить $c$ из $S$
}
\caption{Присвоение элементов кластерам}
\label{alg1}
\end{algorithm}
\efullwidth

На III этапе кластер сохраняется только в случае, если его сходство со всеми ранее полученными кластерами ниже установленного порогового значения.
\parindent=0,5cm

Согласно дистрибутивной гипотезе (Distributional Hypothesis) \cite{Harris 1985} слова, употребляемые в сходных контекстах, близки по смыслу. Алгоритм \textit{Кластеризации посредством комитетов} \cite{Pantel 2002} разрешает лексическую многозначность, группируя слова согласно сходству их контекстов. Каждому полученному кластеру соответствует одно из значений слова.
\parindent=0,5cm

В Табл.~\ref{yank} каждая запись показывает кластеры, которым принадлежит заглавное слово. Имена для кластеров Nq34, Nq137, $\ldots$ генерируются автоматически. После каждого имени кластера находится число, обозначающее сходство между кластером и заглавным словом (т.е. рукав, сердце и одежда). Далее перечисляются четыре слова, наиболее близкие центроиду кластера. Каждый кластер соответствует одному значению заглавного слова. Например, Nq34 соответствует значению <<деталь одежды>>, а Nq137 соответствует значению <<ответвление русла реки>>.

\begin{table}[H]
\caption{Для построения кластеров использовались данные словарных статей Викисловаря: <<одежда>>,  <<рукав>> и  <<сердце>>}
\begin{tabular}{|c|c|m{5cm}|}
\hline
\multicolumn{3}{|c|}{Рукав}\\
\hline 
Nq34 & 0.39 & деталь, манжета, полотно\\
Nq137 & 0.20 & протока, русло, отмель, поток\\
Nq217 & 0.18 & шланг, труба, огнетушитель\\
\hline
\multicolumn{3}{|c|}{Сердце}\\
\hline
Nq72 &  0.27 & орган, костный мозг, почка\\
Nq866 & 0.17 & душа, рассудок, сознание\\
\hline
\multicolumn{3}{|c|}{Одежда}\\
\hline
Nq215 & 0.41 & мануфактура, юбка, брюки\\
Nq235 & 0.20 & покрытие, оболочка, дорога\\
\hline
\end{tabular}
\label{yank}
\end{table}

\textbf{Сравнение с алгоритмом UNICON.} CBC является разновидностью алгоритма UNICON \cite{Lin 2001}, который также строит центроид кластера, используя небольшой набор похожих элементов.
\parindent=0,5cm

Одним из основных различий между UNICON и CBC является то, что UNICON гарантирует, что различные комитеты не имеют одинаковых элементов, тем не менее, центры тяжести двух комитетов по-прежнему могут быть очень близкими (похожими). В~UNICON'е эта проблема решается объединением таких кластеров. В отличие от этого, на II этапе CBC создаются только те комитеты, центры тяжести которых отличны от всех ранее созданных комитетов.
\parindent=0,5cm

Есть разница и на III этапе CBC. Алгоритм UNICON плохо работает со словами, которые имеют несколько широко используемых (доминирующих) значений. Например, пусть значение <<отмычка>> является более употребимым для слова <<ключ>>, чем значение <<водный источник>>. Приведём смесь слов-синонимов к разным значениям слова <<ключ>>: пневмоключ, электроключ, родник, родничок, источник, криница, гидроключ, ключик, тангента, треншальтер, тумблер, знак, контролька, отпирка, виброплекс, шифр. В этом списке 10 значений относятся к значению <<отмычка()>>, 4 к <<водный источник()>> и 2 к значению <<знак()>>. По этому списку алгоритмом UNICON будут сгенерированы кластеры <<отмычка>>, <<шифрование>>, <<происхождение>>,<<криптография>>, <<кнопка>>, <<водный источник>>,<<переключатель>>, <<намёк>>. Сходство между словом и полученными кластерами  является очень низким, к тому же есть кластеры, содержащие одинаковые слова. С другой стороны, CBC удаляет <<пересекающиеся>> (общие для двух кластеров) характеристики после того, как присвоит значение кластеру (допустим характеристики, относящиеся к значению <<отмычка>> слова <<ключ>> из вектора характеристик <<отмычка>>). В результате, сходство между  кластером <<водный источник>> \{родник, родничок, источник, криница\} и пересмотренным вектором характеристик кластера <<водный источник>> становится намного выше. Что в свою очередь приводит к тому, что кластеры становятся гораздо точнее.

\bfullwidth
\begin{center}
\section{WSD-методы, основанные на знаниях}
\end{center}
\efullwidth

\section{Построение сочетаемостных ограничений на основе байесовских сетей для разрешения многозначности}
\begin{flushright}
\textit{И. А. Сихонина} 
\end{flushright}

В статье \cite{Ciaramita 2000} представлена байесовская модель, применяемая для разрешения лексической многозначности глаголов. Авторы рассматривают такое понятие, как сочетаемостные ограничения (selectional preferences). \textit{Сочетаемостные ограничения} (далее SP) --- это закономерности использования глагола относительно семантического класса его параметров (субъект, объект (прямое дополнение) и косвенное дополнение).
\parindent=0,5cm

Модели автоматического построения SP важны сами по себе и имеют приложения в обработке естественного языка. Сочетаемостные ограничения глагола могут применяться для получения возможных значений неизвестного параметра при известных глаголах; например, из предложения \textit{<<Осенние хххх жужжали и бились на стекле>>} легко определить, что “xxxx” --- мухи. При построении предложения SP позволяют отранжировать варианты и выбрать лучший среди них. Исследование SP могло бы помочь в понимании структуры ментального лексикона. 
\parindent=0,5cm

Системы обучения SP без учителя обычно комбинируют статистические подходы и подходы, основанные на знаниях. Компонент базы знаний (здесь WordNet \cite{Miller 1990}) --- это обычно база данных, в которой слова сгруппированы в классы. 
\parindent=0,5cm

Статистический компонент состоит из пар предикат-аргумент, извлечённых из неразмеченного корпуса. В тривиальном алгоритме можно было бы получить список слов (прямых дополнений глагола), и для тех слов, которые есть в WordNet, вывести их семантические классы. В работе \cite{Ciaramita 2000} семантическим классом называется synset (группа синонимов) тезауруса WordNet, то есть класс соответствует одному из значений слова. Таким образом, в тривиальном алгоритме на основе данных WordNet можно выбрать классы (значения слов), с которыми употребляются (встречаются в корпусе) глаголы.
\parindent=0,5cm

Например, если в исходном корпусе текстов глагол \textit{ползать} употребляется со словом \textit{ящерица}, принадлежащим классу РЕПТИЛИИ, то в модели построения SP будет записано, что <<глагол \textit{ползать} употребляется со словами из класса РЕПТИЛИИ>>. Если слово \textit{крокодил}, во-первых, также встречается в тексте с глаголом \textit{ползать}, во-вторых, слово \textit{крокодил} принадлежит сразу двум классам: РЕПТИЛИЯ и ВЕРТОЛЁТ, то из этого следует, что модель SP будет расширена информацией о том, что «глагол \textit{ползать} употребляется со словами из классов и РЕПТИЛИЯ, и ВЕРТОЛЁТ».
\parindent=0,5cm

В ранее разработанных моделях (Резник (1997) \cite{Resnik 1997}, Абни и Лайт (1999) \cite{Abney 1999}) было обнаружено, что главная трудность в таком тривиальном алгоритме --- это наличие неоднозначных слов в обучающих данных. В тех же работах (\cite{Resnik 1997}, \cite{Abney 1999}) были предложены более сложные модели, в которых предполагается, что все значения многозначных слов появляются с одинаковой частотой. 
\parindent=0,5cm

\textbf{байесовские сети} или байесовские сети доверия (БСД) состоят из множества переменных (вершин) и множества ориентированных ребер, соединяющих эти переменные. Такой сети соответствует ориентированный ацикличный граф. Каждая переменная может принимать одно из конечного числа взаимоисключающих состояний. Пусть все переменные будут бинарного типа, то есть принимают одно из двух значений: истина или ложь. Любой переменной \textit{А} с родителями \textit{В1, …, Вn} соответствует таблица условных вероятностей (conditional probability table, далее CPT).
\parindent=0,5cm

Например, построим SP для глагола \textit{ползать} и сеть на Рис. ~\ref{ris1} будет базой знаний.
\begin{figure}[H]
    \includegraphics[keepaspectratio=true,width=0.9\columnwidth]{fig_1.jpg}
    \caption{байесовская сеть для многозначного существительного \textit{крокодил}}
    \label{ris1}
\end{figure}
\parindent=0,5cm

Глагол \textit{ползать} употребляется со словами \textit{крокодил} и \textit{ящерица}. Переменные ВЕРТОЛЁТ и РЕПТИЛИЯ соответствуют более общим абстрактным значениям, переменные \textit{крокодил} и \textit{ящерица} являются более узкими, конкретными значениями. Переменная РЕПТИЛИЯ может принимать одно из двух значений, соответствующих словам \textit{крокодил} и \textit{ящерица}, именно эту задачу определения значения и нужно решить.

\begin{table}[H]
\centering
\caption{Условные вероятности переменных \textit{крокодил} и \textit{ящерица} в зависимости от значений переменных ВЕРТОЛЁТ и РЕПТИЛИЯ, где (В, Р, к, я --- это аббревиатуры слов ВЕРТОЛЁТ, РЕПТИЛИЯ, \textit{крокодил} и \textit{ящерица})}
\begin{tabular}{|c|c|c|c|c|}
\hline
" " & \multicolumn{4}{|c|}{\textit{Р}(\textit{X = x}|\textit{Y_1 = y_1, Y_2 = y_2})} \\
\hline
" " & В,Р & В,\textlnot Р & \textlnot В,Р & \textlnot В, \textlnot Р\\
\hline
к = \textit{true} & 0,99 & 0,99 & 0,99 & 0,01\\
к = \textit{false} & 0,01 & 0,01 & 0,01 & 0,99\\
\hline
я = \textit{true} & 0,99 & 0,99 & 0,01 & 0,01\\
я = \textit{false} & 0,01 & 0,01 & 0,99 & 0,99\\
\hline
\end{tabular}
\label{tbl1}
\end{table}

\parindent=0,5cm
При построении Табл. ~\ref{tbl1} условных вероятностей (CPT), учтём следующие предположения:

\begin{itemize}
\item вероятность, что выбираем какой-либо из концептов (ВЕРТОЛЁТ и РЕПТИЛИЯ) очень мала, то есть \textit{P}(\textit{В=true}) = \textit{P}(\textit{Р=true}) = 0,01, следовательно, велика вероятность, что концепты не выбраны: \textit{P}(\textit{В=false}) = \textit{P}(\textit{Р= false}) = 0,99;
\item если какой-либо из концептов истинен (В, Р), то <<выпадает>> слово \textit{крокодил};
\item если концепт РЕПТИЛИЯ истинен, то растут шансы встретить слово \textit{ящерица};
\end{itemize}
\parindent=0,5cm

Из Табл. ~\ref{tbl1} вероятности появления слов следует вывод, что использование разу двух значений слова \textit{крокодил (рептилия и вертолёт МИ-24)} маловероятно. Вероятность использования значения РЕПТИЛИЯ намного больше чем значения ВЕРТОЛЁТ. Таким образом гипотеза <<вертолёт>> <<отброшена>> (“explaining away”).
\parindent=0,5cm

\textbf{байесовские сети для построения SP.} Иерархия существительных в WordNet представлена в виде ориентированного ацикличного графа. Синсет узла принимает значение <<истина>>, если глагол <<выбирает>> существительное из набора синонимов. Априорные вероятности задаются на основе двух предположений: во-первых, маловероятно, что глагол будет употребляться только со словами какого-то конкретного synset'а, и во-вторых, если глагол действительно употребляется только со словами из данного synset'а (например, synset ЕДА), тогда должно быть правомерным употребление этого глагола с гипонимами этого synset'а (например, ФРУКТ).
\parindent=0,5cm

Те же предположения (что для synset'ов) верны и для употреблений слов с глаголами:
\begin{enumerate}
\item слово, вероятно, является аргументом глагола в том случае, если глагол употребляется с каким-либо из значений этого слова;
\item отсутствие связки глагол-synset говорит о малой вероятности того, что слова этого synset'а употребляются с глаголом;
\end{enumerate}
\parindent=0,5cm

Словам <<вероятно>> и <<маловероятно>> должны быть приписаны такие числа, сумма которых равна единице. 
\parindent=0,5cm

Находкой работы \cite{Ciaramita 2000} является разъяснение стратегии “explaining away”, то есть отбрасывание маловероятных значений слов при построении сочетаемостных ограничений. Такая стратегия является неотъемлемым свойством байесовских сетей и байесовского вывода, полезным свойством при разрешении лексической многозначности. 
 
\begin{thebibliography}{9}

\bibitem{Averin 2006}
\textit{Аверин, А.Н.} Разработка сервиса поиска биграмм // Труды международной конференции «Корпусная лингвистика–2006. СПб., С.Петерб. ун-та., 2006.

\bibitem{Kim 1989}
\textit{Ким Дж. О., Мьюллер Ч.У., Клекка У.Р.} 1989.Факторный, дискриминантный и кластерный анализ. Стр.172. <<Финансы и статистика>>, Москва, Россия.

\bibitem{epr:website}
\textit{Епрев, А. С.}Применение контекстных векторов в классификации текстовых документов. 2010. http://jre.cplire.ru/iso/oct10/1/text.html

\bibitem{Abney 1999}
\textit{Abney, S. and Light, M.}Hiding a semantic hierarchy in a markov model. In Proceedings of the Workshop on Unsupervised Learning in Natural Language Processing, ACL. 1999.

\bibitem{Berry 1993}
\textit{M. Berry, T. Do, G. O’Brien, V. Krishna, and S. Varadhan}SVDPACK (version 1.0) user’s guide. Technical Report CS-93-194, University of Tennessee at Knoxville, Computer Science Department, April 1993.

\bibitem{Bruce 1994}
\textit{Bruce, R. and Wiebe, J.}Word-sense disambiguation using decomposable models. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, 1994, pages 139–146.

\bibitem{Ciaramita 2000}
\textit{Ciaramita, M. and Johnson, M.}Explaining away ambiguity: Learning verb selectional preference with Bayesian networks. 2000. 

\bibitem{COTTRELL 1983}
\textit{COTTRELL, G. W. and SMALL, S. L.}A connectionist  scheme for modelling word  sense  disambiguation -- Cognition and brain theory. 1983. № 6. P. 89–120. 

\bibitem{Charles 1993}
\textit{Charles X. Ling, Marin Marinov}Answering the connectionist challenge: A symbolic model of learning the past tenses of English verbs,  Cognition, Elsevier, 1993

\bibitem{Dekang 1998}
\textit{Dekang Lin.}1998. Automatic Retrieval and Clustering of Similar Words.Proceedings of the 17th international conference on Computational linguistics-Volume 2. – Association for Computational Linguistics, pages 768-774. Department of Computer Science University of Manitoba Winnipeg, Manitoba, Canada

\bibitem{Dekang 1993}
\textit{Dekang Lin.}1993. Principle-based parsing without overgeneration. In Proceedings of ACL-93, pages 112-120, Columbus, Ohio.

\bibitem{Dekang 1997}
\textit{Dekang Lin.}1997. Using syntactic dependency as local context to resolve word sense ambiguity. In Proceedings of ACL/EACL-97, pages 64-71, Madrid, Spain, July.

\bibitem{Donald 1990}
\textit{Donald Hindle.}1990. Noun classification from predicate-argument structures. In Proceedings of ACL-90, pages 268-275, Pittsburg, Pennsylvania, June.

\bibitem{Eugene 1975}
\textit{Eugene A. Nida.}1975. Componential Analysis of Meaning. The Hague, Mouton.

\bibitem{Harris 1985}
\textit{Harris, Z.}Distributional structure. In: Katz, J. J. (ed.) The Philosophy of Linguistics. New York: Oxford University Press. 1985. pp. 26–47

\bibitem{Jain 1988}
\textit{Jain, A. and Dubes, R.}Algorithms for Clustering Data. Prentice-Hall, Inc., Upper Saddle River, NJ, 1988.

\bibitem{Jain 1999}
\textit{A. Jain, M. Murthy, and P. Flynn}Data clustering: a review. ACM Computing Surveys, 31(3):264-323, September 1999.

\bibitem{Leacock 1993}
\textit{C. Leacock, G. Towell, and E. Voorhees.}Corpus-based statistical sense resolution. In Proceedings of the ARPA Workshop on Human Language Technology, pages 260–265, March. 1993. 

\bibitem{LESK 1986}
\textit{LESK, M.}Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone --  Proceedings  of  the 5th SIGDOC. New York. 1986. P. 24–26. 

\bibitem{Lin 2001}
\textit{Lin, D. and Pantel, P}2001. Induction of semantic classes from natural language text. In Proceedings of SIGKDD-01. pp. 317–322. San Francisco, CA  

\bibitem{Manning 1999}
\textit{Manning, C. D. and Sch?tze, H.}1999. Foundations of Statistical Natural Language Processing. MIT Press.

\bibitem{Miller 1990}
\textit{Miller, G.}Wordnet: An on-line lexical database. International Journal of Lexicography, 3(4). 1990.

\bibitem{Mooney 1996}
\textit{Raymond J. Mooney}Comparative Experiments on Disambiguating Word Senses:
An Illustration of the Role of Bias in Machine Learning, Department of Computer Sceinces, University of Texas, Austin, TX 78712-1188, 1996

\bibitem{Mooney 1995}
\textit{R. J. Mooney, M. E. Califf}Induction of First-Order Decision Lists: Results on Learning the Past Tense of English Verbs, Department of Computer Sceinces, University of Texas, Austin, TX 78712-1188, 1995

\bibitem{Pantel 2002}
\textit{Patrick Pantel, Dekang Lin}Discovering Word Senses from Text. University of Alberta. Department of Computing Science Edmonton, Alberta T6H 2E1 Canada, 2002.

\bibitem{Pedersen 2000}
\textit{Pedersen, T.}A Simple Approach to Building Ensembles of Naive Bayesian Classi?ers for Word Sense Disambiguation// Department of Computer Science, University of Minnesota Duluth.–2000.

\bibitem{Pedersen 1997}
\textit{Pedersen, T. and Bruce, R.}Distinguishing word senses in untagged text. Proc.EMNLP.Providence, RI, 1997.

\bibitem{Purandare 2004}
\textit{Purandare, A. and Pedersen, T.}Improving word sense discrimination with gloss augmented feature vectors // Workshop on Lexical Resources for the Web and Word Sense Disambiguation. – 2004. – С. 123-130. 

\bibitem{Resnik 1997}
\textit{Resnik, P.}Selectional preference and sense disambiguation. In Proceedings of the ANLP-97 Workshop: Tagging Text with Lexical Semantics: Why, What, and How? 1997.

\bibitem{Savova 2005}
\textit{Savova, G.} Resolving ambiguities in biomedical text with unsupervised clustering approaches. University of Minnesota Supercomputing Institute Research Report, 2005.

\bibitem{VERONIS 1990}
\textit{VERONIS, J. and IDE, N.} Word  sense  disambiguation  with  very  large neural  networks  extracted  from machine readable dictionaries – Proceedings of the 13th International Conference on Computational Linguistics. Helsinki. 1990. P. 389–394.

\bibitem{SC:website}
\textit{SenseClusters}http://senseclusters.sourceforge.net

\bibitem{Schutze 1998}
\textit{Schutze, H.}Automatic Word Sense Discrimination. Computational Linguistics, vol. 24, number 1., 1998.

\bibitem{UMLS:website}
\textit UMLS Terminology Services (UTS). http://umlsks.nlm.nih.gov/kss/servlet/Turbine/template

\bibitem{WALTZ 1985}
\textit{WALTZ, D. L. and POLLACK, J. B.}Massively parallel parsing: a strongly interactive  model  of  natural  language interpretation – Cognitive science. 1985. № 9. P. 51–74.

\bibitem{Weeber 2001}
\textit{Weeber, M. and Mork, J. and Aronson, A.}Developing a test collection for biomedical word sense disambiguation. Proc. AMIA., 2001.

\bibitem{Zhao 2002}
\textit{Zhao, Y. and Karypis, G.} Evaluation of hierarchical clustering algorithms for document datasets. In Proceedings of the 11th International Conference on Information and Knowledge Management, pages 515-524, McLean, VA, 2002.



\end{thebibliography}
\end{articletext}


\section{СВЕДЕНИЯ ОБ АВТОРE:}

\begin{aboutauthors}
\authorsname{Кириллов Александр Николаевич}
доктор физико-математических наук\\ 
доцент\\
Институт прикладных математических исследований КарНЦ РАН\\ 
ул. Пушкинская, 11, Петрозаводск, Республика Карелия, Россия, 185910\\
эл. почта: kirillov@krc.karelia.ru\\
тел.: (8142) 766312

\columnbreak

\authorsname{Kirillov, Alexander}
Doctor (DSc) of Physics and Mathematics\\
Assistant Professor\\
Institute of Applied Mathematical Research, Karelian Research Centre, Russian Academy of Science\\
11 Pushkinskaya St., 185910 Petrozavodsk, Karelia, Russia\\
e-mail: kirillov@krc.karelia.ru\\
tel.: (8142) 766312
\end{aboutauthors}

\begin{aboutauthors}
\authorsname{Коржицкий Никита Иванович}
Студент\\
Математический факультет\\ 
Петрозаводский государственный университет\\
пр-кт Ленина, 33, Петрозаводск, Республика Карелия\\
тел.: +7(8142) 71-10-78\\
nikita@nikita.tv

\columnbreak

\authorsname{Korzhitsky, Nikita}
Student\\
Faculty of Mathematics\\
Petrozavodsk State University\\
Prospect Lenina, 33, Petrozavodsk, Republic of Karelia\\
tel.: +7(8142) 71-10-78\\
nikita@nikita.tv 
\end{aboutauthors}

\begin{aboutauthors}
\authorsname{Крижановский Андрей Анатольевич}
кандитат технических наук\\ 
Институт прикладных математических исследований КарНЦ РАН\\ 
ул. Пушкинская, 11, Петрозаводск, Республика Карелия, Россия, 185910\\
эл. почта: andew.krizhanovsky@gmail.com\\
тел.: (8142) 766312

\columnbreak

\authorsname{Krizhanovsky, Andrew}
Cand. (PhD) of Techics\\
Institute of Applied Mathematical Research, Karelian Research Centre, Russian Academy of Science\\
11 Pushkinskaya St., 185910 Petrozavodsk, Karelia, Russia\\
e-mail: andew.krizhanovsky@gmail.com\\
tel.: (8142) 766312
\end{aboutauthors}

\begin{aboutauthors}
\authorsname{Сихонина Ирина Александровна}
Студентка\\
Математический факультет\\ 
Петрозаводский государственный университет\\
пр-кт Ленина, 33, Петрозаводск, Республика Карелия\\
тел.: +7(8142) 71-10-78\\
syawenka@mail.ru

\columnbreak

\authorsname{Sikhonina, Irina}
Student\\
Faculty of Mathematics\\
Petrozavodsk State University\\
Prospect Lenina, 33, Petrozavodsk, Republic of Karelia\\
tel.: +7(8142) 71-10-78\\
syawenka@mail.ru 
\end{aboutauthors}

\begin{aboutauthors}
\authorsname{Спиркова Анна Михайловна}
Студентка\\
Математический факультет\\ 
Петрозаводский государственный университет\\
пр-кт Ленина, 33, Петрозаводск, Республика Карелия\\
тел.: +7(8142) 71-10-78\\
annspirkova@gmail.com

\columnbreak

\authorsname{Spirikova, Anna}
Student\\
Faculty of Mathematics\\
Petrozavodsk State University\\
Prospect Lenina, 33, Petrozavodsk, Republic of Karelia\\
tel.: +7(8142) 71-10-78\\
annspirkova@gmail.com
\end{aboutauthors}

\begin{aboutauthors}
\authorsname{Ткач Станислав Сергеевич}
Студент\\
Математический факультет\\ 
Петрозаводский государственный университет\\
пр-кт Ленина, 33, Петрозаводск, Республика Карелия\\
тел.: +7 (8142) 71-10-78\\
tkachkras@gmail.com

\columnbreak

\authorsname{Tkach, Stanislav}
Student\\
Faculty of Mathematics\\
Petrozavodsk State University\\
Prospect Lenina, 33, Petrozavodsk, Republic of Karelia\\
tel.: +7 (8142) 71-10-78\\
tkachkras@gmail.com 
\end{aboutauthors}

\begin{aboutauthors}
\authorsname{Чухарев Алексей Леонидович}
старший инженер-программист\\ 
Институт прикладных математических исследований КарНЦ РАН\\ 
ул. Пушкинская, 11, Петрозаводск, Республика Карелия, Россия, 185910\\
тел.: (8142) 766312

\columnbreak

\authorsname{Chuharev, Alexey}
Senior Programming Engineer\\
Institute of Applied Mathematical Research, Karelian Research Centre, Russian Academy of Science\\
11 Pushkinskaya St., 185910 Petrozavodsk, Karelia, Russia\\
tel.: (8142) 766312
\end{aboutauthors}

\begin{aboutauthors}
\authorsname{Шорец Дарья Сергеевна}
Студентка\\ 
Математический факультет\\ 
Петрозаводский государственный университет\\
пр-кт Ленина, 33, Петрозаводск, Республика Карелия\\
+7 (8142) 71-10-78\\
da\_sha1078@mail.ru

\columnbreak

\authorsname{Shorets, Daria}
Student\\
Faculty of Mathematics\\
Petrozavodsk State University\\
Prospect Lenina, 33, Petrozavodsk, Republic of Karelia\\
+7 (8142) 71-10-78\\
da\_sha1078@mail.ru
\end{aboutauthors}

\begin{aboutauthors}
\authorsname{Ярышкина Екатерина Александровна}
Студентка\\ 
Математический факультет\\ 
Петрозаводский государственный университет\\
пр-кт Ленина, 33, Петрозаводск, Республика Карелия\\
+7 (8142) 71-10-78\\
kate.rysh@gmail.com

\columnbreak

\authorsname{Yaryshkina, Ekaterina}
Student\\
Faculty of Mathematics\\
Petrozavodsk State University\\
Prospect Lenina, 33, Petrozavodsk, Republic of Karelia\\
+7 (8142) 71-10-78\\
kate.rysh@gmail.com 
\end{aboutauthors}

\begin{aboutauthors}
\authorsname{Янкевич Дарья Юрьевна}
Студентка\\ 
Математический факультет\\ 
Петрозаводский государственный университет\\
пр-кт Ленина, 33, Петрозаводск, Республика Карелия\\
+7 (8142) 71-10-78\\

\columnbreak

\authorsname{Yankevich, Daria}
Student\\
Faculty of Mathematics\\
Petrozavodsk State University\\
Prospect Lenina, 33, Petrozavodsk, Republic of Karelia\\
+7 (8142) 71-10-78\\ 
\end{aboutauthors}

\end{document}
