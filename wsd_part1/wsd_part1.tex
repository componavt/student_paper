\documentclass{article}
\usepackage{krctran}
\usepackage{textcomp}

\begin{document}

\procname{Труды Карельского научного центра РАН\\ \No 0. 2010. С.~1--4}
\udk{УДК 004.01:006.72 (470.22)}

\rustitle{Обзор методов (уточнить - каких?) \newline по решению задачи разрешения лексической многозначности}
\engtitle{Word-sense disambiguation methods (specific?) review}

\rusauthor{С.~С.~Ткач, Е.~А.~Ярышкина, А.~Н.~Кирилов, И.~А.~Сихонина}
\engauthor{S.~S.~Tkach, E.~A.~Yaryshkina, A.~N.~Kirilov, I.~A.~Sikhonina}


\organization{Институт прикладных математических исследований Карельского научного центра РАН}

\rusabstract{Данный файл является примером статьи для научного издания Труды Карельского научного центра РАН, 
серия <<Математическое моделирование и информационные технологии>>.
В нем содержатся основные используемые переменные и окружения. При подготовке статьи рекомендуется
воспользоваться этим примером в качестве шаблона. Данный абзац оформлен в стиле аннотации статьи.}
\engabstract{This file is an auxiliary example of an article prepared forTransactions of Karelian Research Centre of RAS.
It contains most useful environments and variables. While preparing your article, it's recommended to use
this text as a template. This paragraph is formatted as an Abstract of the article.}

\ruskeywords{труды, шаблон, подготовка статьи.}
\engkeywords{transactions, template, article.}

\maketitle

\begin{articletext}
\section{Введение}
Статья, представляемая в научное издание, должна быть оформлена в соответствии
с <<Правилами для авторов>>, размещенными на сайте \hbox{http://transactions.krc.karelia.ru}.
Данный документ претендует на роль технической документации в помощь авторам.
Достаточно подробную информацию по наборе в системе \LaTeX можно найти,
напр., в работе~\cite{Latexrus}.

\section{Структура файла в формате \LaTeXe}

\begin{verbatim}
\documentclass{article}
\usepackage{krctran}
...
\begin{document}

\procname{...}
\udk{...}
\rustitle{...}
\engtitle{...}
\rusauthor{...}
\engauthor{...}
\organization{...}
\rusabstract{...}
\engabstract{...}
\ruskeywords{...}
\engkeywords{...}

\maketitle

\begin{articletext}
\section{...}
...
\begin{thebibliography}
...
\end{thebibliography}
\end{articletext}

\section{СВЕДЕНИЯ ОБ АВТОРE:}
\begin{aboutauthors}
...
\end{aboutauthors}
\end{document}
\end{verbatim}

Преамбула статьи должна содержать две обязательные команды:
\begin{verbatim}
\documentclass{article}
\usepackage{krctran}
\end{verbatim}

Далее формируется заголовок статьи: выходные данные, код УДК, название статьи, 
авторы с указанием мест работы, аннотация,
ключевые слова. Например, заголовок этого файла сформирован следующими командами: 
\begin{verbatim}
\procname{Труды...\\ \No...}
\udk{УДК...}
\rustitle{Руководство...}
\engtitle{Usage...}
\rusauthor{А.~С.~Румянцев}
\engauthor{A.~S.~Rumyantsev}
\organization{Институт...}
\rusabstract{Данный файл...}
\engabstract{This file...}
\ruskeywords{труды,...}
\engkeywords{transactions,...}
\maketitle
\end{verbatim}

\begin{Remark}
Для ручной разбивки на строки названия статьи воспользуйтесь командой \verb"\newline".
\end{Remark}

\bfullwidth
\begin{figure}
%\includegraphics[height=100mm,width=0.9\textwidth]{delay_HT.pdf}
\caption{Задержки в модели 10-узловой системы, случай тяжелых хвостов}
\label{fig5}
\end{figure}
\efullwidth

\section{WSD-методы с учителем}

\section{<<WSD на основе нейронных сетей, построенных по данным машиночитаемых словарей>>}

\begin{flushright}
\textit{А. Н. Кирилов}
\end{flushright}

\parindent=0,5cm

Настоящий текст является рефератом статьи \cite{VERONIS 
1990}, в которой описан метод автоматического построения очень больших нейронных сетей (VLNN) с помощью текстов, извлекаемых из машинно-читаемых словарей (MRD), и рассмотрено использование этих сетей в задачах разрешения лексической неоднозначности (WSD). 

\parindent=0,5cm

В дальнейшем будем называть слова, смысл которых требуется установить целевыми словами. 

\parindent=0,5cm

Широко известен метод Леска \cite{LESK 1986}  использования  информации  из MRD для  задачи  WSD.  Суть этого метода состоит в вычислении  так называемой <<степени пересечения>>, т.е.  количества общих слов в словарных определениях слов из контекста (“окна”) условного размера, содержащего целевое слово. Основной недостаток метода Леска -- зависимость  от  словарной  статьи, т.е. от слов, входящих в нее. Стратегия преодоления  этого  недостатка -- использование словарных статей, определяющих слова, входящие в другие словарные статьи, начиная со словарных статей, соответствующих  словам  из  контекста. Таким образом, образуются  достаточно  длинные   пути  из слов, входящих в словарные статьи. Эта  идея  лежит в основе топологии (строения) VLNN.

\parindent=0,5cm

Использование нейронных сетей  для WSD  было предложено в работах \cite{COTTRELL 1983,WALTZ 1985}.   В рассматриваемой  статье для построения  VLNN использован  словарь  Collins  English  Dictionary.

\parindent=0,5cm

Топология сети. Целевое слово представлено  узлом, соединенным активирующими  связями со смысловыми узлами, представляющими все возможные смыслы слова, имеющиеся  в словарных статьях. Каждый смысловой узел, в свою очередь, соединен  активирующими  связями с узлами, представляющими  слова в словарной статье, соответствующей определению данного смысла. Процесс соединения повторяется многократно, создавая большую сеть взаимосвязанных узлов. В идеале сеть может содержать весь словарь. Авторы, по практическим соображениям, ограничиваются несколькими тысячами узлов и 10 -- 20 тысячами соединений. Слова представлены своими леммами (каноническими формами). Узлы, представляющие различные смыслы данного слова, соединены запрещающими (подавляющими) связями.

\parindent=0,5cm

Алгоритм функционирования сети. При запуске сети первыми активируются узлы входного  слова, которое кодируется согласно принятому правилу. Затем  каждый  входной узел посылает активирующий сигнал своим смысловым узлам, с которыми он соединен. В результате  сигналы распространяются по всей сети в течение определенного числа циклов. В каждом цикле узлы слова и  его смыслов получают обратные сигналы от узлов, соединенных с ними. Узлы конкурирующих смыслов посылают взаимно подавляющие сигналы.  Взаимодействие  сигналов  обратной  связи  и  подавления,  в соответствии со стратегией <<победитель получает все>>, позволяет увеличить активацию узлов-слов и соответствующих им правильных узлов-смыслов,  одновременно  уменьшая  активацию  узлов  соответствующих неправильным смыслам. После нескольких десятков  циклов сеть стабилизируется  в  состоянии, в котором активированы  только узлы-смыслы с наиболее активированными связями  с    узлами-словами. В статье  не  указан  алгоритм  настройки, т.е.  обучения  сети.  Видимо,  используется  метод  встречного  распространения (back  propagation).

\section{WSD-методы без учителя}

\section{<<Разрешение многозначности в биомедицинских текстах с помощью методов кластеризации без учителя>>}

\begin{flushright}
\textit{Е. А. Ярышкина} 
\end{flushright}

В статье \cite{Savova 2005} изучаются уже существующие методы кластеризации без учителя и их эффективность для решения лексической многозначности при обработке текстов по биомедицине. Решение проблем лексической многозначности в данной области включает в себя не только традиционные задачи присвоения ранее определенных смысловых значений для терминов, но так же и обнаружения новых значений для них, ещё не включённых в данную онтологию.
\parindent=0,5cm

Авторы описали методологию метода решения лексической многозначности без учителя, учитываемые лексические признаки и наборы экспериментальных данных. В качестве оценки эффективности алгоритмов кластеризации текста была предложена F-мера. 
\parindent=0,5cm

Подход для решения поставленной задачи -- это разделение контекстов (фрагментов текста), содержащих определенное целевое слово на кластеры, где каждый кластер представляет собой различные значения целевого слова. Каждый кластер состоит из близких по значению контекстов. Задача решается в предположении, что используемое целевое слово в аналогичном контексте будет иметь один и тот же или очень похожий смысл. 
\parindent=0,5cm

Процесс кластеризации продолжается до тех пор, пока не будет найдено предварительно заданное число кластеров. В данной статье выбор шести кластеров основан на том факте, что это больше, чем максимальное число возможных значений любого английского слова, наблюдаемое среди данных (большинство слов имеют два-три значения). Нормализация текста не выполняется.
\parindent=0,5cm

Данные в этом исследовании состоят из ряда контекстов, которые включают данное целевое слово, где у каждого целевого слова вручную отмечено -- какое значение из словаря было использовано в этом контексте. Контекст -- это единственный источник информации о целевом слове. Цель исследования -- преобразовать контекст в контекстные вектора первого и второго порядка \cite{epr:website}. Контекстные вектора содержат следующие «лексические свойства»: биграммы, совместную встречаемость и совместную встречаемость целевого слова. Биграммами являются как двухсловные словосочетания, так и любые два слова, расположенные рядом в некотором тексте. Для лингвистических исследований могут быть полезны только упорядоченные наборы биграмм \cite{Averin 2006}. 
\parindent=0,5cm

Экспериментальные данные -- это набор NLM WSD \cite{UMLS:website} (NLM -- национальная библиотека медицины США), в котором значения слов взяты из UMLS (единая система медицинской терминологии). UMLS имеет три базы знаний: 
\begin{itemize}
\item Метатезаурус включает все термины из контролируемых словарей (SNOMED-CT, ICD и другие) и понятия, которые представляют собой кластера из терминов, описывающих один и тот же смысл. 
\item Семантическая сеть распределяет понятия на 134 категории и показывает отношения между ними. SPECIALIST-лексикон содержит семантическую информацию для терминов Метатезауруса. 
\item Medline -- главная библиографическая база данных NLM, которая включает приблизительно 13 миллионов ссылок на журнальные статьи в области науки о жизни с уклоном в биомедицинскую область.
\end{itemize}
\parindent=0,5cm

Авторы успешно проверили по три конфигурации существующих методов (PB -- Pedersen and Bruce \cite{Pedersen 1997}, SC -- Sch?tze \cite{Schutze 1998}) и оценили эффективность использования SVD (сингулярное разложение матриц). Методы PB основаны на контекстных векторах первого порядка -- признаки одновременного присутствия целевого слова или биграммы. Рассчитывается среднее расстояние между кластерами или применяется  метод бисекций. PB методы подходят для работы с довольно большими наборами данных. Методы SC основаны на представлениях второго порядка -- матрицы признаков одновременного присутствия или биграммы, где каждая строка и столбец -- вектор признаков первого порядка данного слова. Так же рассчитывается среднее расстояние между кластерами или применяется  метод бисекций. SC методы подходят для обработки небольших наборов данных.  

\parindent=0,5cm
Метод SC2 (признаки одновременного присутствия второго порядка, среднее расстояние между элементами кластера в пространстве подобия) с применением и без SVD показал лучшие результаты: всего 56 сравниваемых экземпляров, в 47 случаях метод SC2 показал наилучшие результаты, в 7 случаях результаты незначительно отличаются от других проверяемых методов.
\parindent=0,5cm

Все эксперименты, указанные в исследовании, выполнялись с помощью пакета SenseClusters \cite{SC:website}. В ходе исследования было проведено два эксперимента для разных наборов данных. Маленький тренировочный набор -- это набор NLM WSD, который включает 5000 экземпляров для 50 часто встречаемых неоднозначных терминов из Метатезауруса UMLS. Каждый неоднозначный термин имеет по 100 экземпляров с указанным вручную значением. У 21 термина максимальное число экземпляров находится в пределах от 45 до 79 экземпляров. У 29 терминов число экземпляров от 80 до 100 для  конкретного значения. Стоит отметить, что каждый термин имеет категорию «ни одно из вышеупомянутых», которая охватывает все оставшиеся значения, не соответствующие доступным в UMLS. Большой тренировочный набор является реконструкцией «1999 Medline», который был разработан Weeber \cite{Weeber 2001}. Были определены все формы из набора NLM WSD и сопоставлены с тезисами «1999 Medline». Для создания тренировочного набора экземпляров использовались только те тезисы из «1999 Medline», которым было найдено соответствие в наборе NLM WSD.
\parindent=0,5cm

Использование целиком текста аннотации статьи в качестве контекста приводит к лучшим результатам, чем использование отдельных предложений. С одной стороны, большой объем контекста, представленный аннотацией, дает богатую коллекцию признаков, с другой стороны, в коллекции WSD представлено небольшое число контекстов.

\section{WSD-методы, основанные на знаниях}
\section{<<Построение сочетаемостных ограничений на основе Байесовских сетей для разрешения многозначности>>}
\begin{flushright}
\textit{И. А. Сихонина} 
\end{flushright}

В статье \cite{Ciaramita 2000} представлена Байесовская модель, применяемая для разрешения лексической многозначности глаголов. Авторы рассматривают такое понятие, как сочетаемостные ограничения (selectional preferences). \textit{Сочетаемостные ограничения} (далее SP) -- это закономерности использования глагола относительно семантического класса его параметров (субъект, объект (прямое дополнение) и косвенное дополнение).
\parindent=0,5cm

Модели автоматического построения SP важны сами по себе и имеют приложения в обработке естественного языка. Сочетаемостные ограничения глагола могут применяться для получения возможных значений неизвестного параметра при известных глаголах; например, из предложения \textit{<<Осенние хххх жужжали и бились на стекле>>} легко определить, что “xxxx” -- мухи. При построении предложения SP позволяют отранжировать варианты и выбрать лучший среди них. Исследование SP могло бы помочь в понимании структуры ментального лексикона. 
\parindent=0,5cm

Системы обучения SP без учителя обычно комбинируют статистические подходы и подходы, основанные на знаниях. Компонент базы знаний (здесь WordNet \cite{Miller 1990}) -- это обычно база данных, в которой слова сгруппированы в классы. 
\parindent=0,5cm

Статистический компонент состоит из пар предикат-аргумент, извлечённых из неразмеченного корпуса. В тривиальном алгоритме можно было бы получить список слов (прямых дополнений глагола), и для тех слов, которые есть в WordNet, вывести их семантические классы. В работе \cite{Ciaramita 2000} семантическим классом называется синсет (группа синонимов) тезауруса WordNet, то есть класс соответствует одному из значений слова. Таким образом, в тривиальном алгоритме на основе данных WordNet можно выбрать классы (значения слов), с которыми употребляются (встречаются в корпусе) глаголы.
\parindent=0,5cm

Например, если в исходном корпусе текстов глагол \textit{ползать} употребляется со словом \textit{ящерица}, принадлежащим классу РЕПТИЛИИ, то в модели построения SP будет записано, что <<глагол \textit{ползать} употребляется со словами из класса РЕПТИЛИИ>>. Если слово \textit{крокодил}, во-первых, также встречается в тексте с глаголом \textit{ползать}, во-вторых, слово \textit{крокодил} принадлежит сразу двум классам: РЕПТИЛИЯ и ВЕРТОЛЁТ, то из этого следует, что модель SP будет расширена информацией о том, что «глагол \textit{ползать} употребляется со словами из классов и РЕПТИЛИЯ, и ВЕРТОЛЁТ».
\parindent=0,5cm

В ранее разработанных моделях (Резник (1997) \cite{Resnik 1997}, Абни и Лайт (1999) \cite{Abney 1999}) было обнаружено, что главная трудность в таком тривиальном алгоритме -- это наличие неоднозначных слов в обучающих данных. В тех же работах (\cite{Resnik 1997}, \cite{Abney 1999}) были предложены более сложные модели, в которых предполагается, что все значения многозначных слов появляются с одинаковой частотой. 
\parindent=0,5cm

\textbf{Байесовские сети} или Байесовские сети доверия (БСД) состоят из множества переменных (вершин) и множества ориентированных ребер, соединяющих эти переменные. Такой сети соответствует ориентированный ацикличный граф. Каждая переменная может принимать одно из конечного числа взаимоисключающих состояний. Пусть все переменные будут бинарного типа, то есть принимают одно из двух значений: истина или ложь. Любой переменной \textit{А} с родителями \textit{В1, …, Вn} соответствует таблица условных вероятностей (conditional probability table, далее CPT).
\parindent=0,5cm

Например, построим SP для глагола \textit{ползать} и сеть на рисунке ~\ref{ris1} будет базой знаний.
\begin{figure}[H]
    \includegraphics[keepaspectratio=true,width=0.9\columnwidth]{fig_1.jpg}
    \caption{Байесовская сеть для многозначного существительного \textit{крокодил}}
    \label{ris1}
\end{figure}
\parindent=0,5cm

Глагол \textit{ползать} употребляется со словами \textit{крокодил} и \textit{ящерица}. Переменные ВЕРТОЛЁТ и РЕПТИЛИЯ соответствуют более общим абстрактным значениям, переменные \textit{крокодил} и \textit{ящерица} являются более узкими, конкретными значениями. Переменная РЕПТИЛИЯ может принимать одно из двух значений, соответствующих словам \textit{крокодил} и \textit{ящерица}, именно эту задачу определения значения и нужно решить.

\begin{table}[H]
\centering
\caption{Условные вероятности переменных \textit{крокодил} и \textit{ящерица} в зависимости от значений переменных ВЕРТОЛЁТ и РЕПТИЛИЯ, где (В, Р, к, я -- это аббревиатуры слов ВЕРТОЛЁТ, РЕПТИЛИЯ, \textit{крокодил} и \textit{ящерица})}
\begin{tabular}{|c|c|c|c|c|}
\hline
" " & \multicolumn{4}{|c|}{\textit{Р}(\textit{X = x}|\textit{Y_1 = y_1, Y_2 = y_2})} \\
\hline
" " & В,Р & В,\textlnot Р & \textlnot В,Р & \textlnot В, \textlnot Р\\
\hline
к = \textit{true} & 0,99 & 0,99 & 0,99 & 0,01\\
к = \textit{false} & 0,01 & 0,01 & 0,01 & 0,99\\
\hline
я = \textit{true} & 0,99 & 0,99 & 0,01 & 0,01\\
я = \textit{false} & 0,01 & 0,01 & 0,99 & 0,99\\
\hline
\end{tabular}
\label{tbl1}
\end{table}

\parindent=0,5cm
При построении таблицы ~\ref{tbl1} условных вероятностей (CPT), учтём следующие предположения:

\begin{itemize}
\item вероятность, что выбираем какой-либо из концептов (ВЕРТОЛЁТ и РЕПТИЛИЯ) очень мала, то есть \textit{P}(\textit{В=true}) = \textit{P}(\textit{Р=true}) = 0,01, следовательно, велика вероятность, что концепты не выбраны: \textit{P}(\textit{В=false}) = \textit{P}(\textit{Р= false}) = 0,99;
\item если какой-либо из концептов истинен (В, Р), то <<выпадает>> слово \textit{крокодил};
\item если концепт РЕПТИЛИЯ истинен, то растут шансы встретить слово \textit{ящерица};
\end{itemize}
\parindent=0,5cm

Из таблицы ~\ref{tbl1} вероятности появления слов следует вывод, что использование разу двух значений слова \textit{крокодил (рептилия и вертолёт МИ-24)} маловероятно. Вероятность использования значения РЕПТИЛИЯ намного больше чем значения ВЕРТОЛЁТ. Таким образом гипотеза <<вертолёт>> <<отброшена>> (“explaining away”).
\parindent=0,5cm

\textbf{Байесовские сети для построения SP.} Иерархия существительных в WordNet представлена в виде ориентированного ацикличного графа. Синсет узла принимает значение <<истина>>, если глагол <<выбирает>> существительное из набора синонимов. Априорные вероятности задаются на основе двух предположений: во-первых, маловероятно, что глагол будет употребляться только со словами какого-то конкретного синсета, и во-вторых, если глагол действительно употребляется только со словами из данного синсета (например, синсет ЕДА), тогда должно быть правомерным употребление этого глагола с гипонимами этого синсета (например, ФРУКТ).
\parindent=0,5cm

Те же предположения (что для синсетов) верны и для употреблений слов с глаголами:
\begin{enumerate}
\item слово, вероятно, является аргументом глагола в том случае, если глагол употребляется с каким-либо из значений этого слова;
\item отсутствие связки глагол-синсет говорит о малой вероятности того, что слова этого синсета употребляются с глаголом;
\end{enumerate}
\parindent=0,5cm

Словам <<вероятно>> и <<маловероятно>> должны быть приписаны такие числа, сумма которых равна единице. 
\parindent=0,5cm

Находкой работы \cite{Ciaramita 2000} является разъяснение стратегии “explaining away”, то есть отбрасывание маловероятных значений слов при построении сочетаемостных ограничений. Такая стратегия является неотъемлемым свойством Байесовских сетей и Байесовского вывода, полезным свойством при разрешении лексической многозначности. 
 

\section{Библиография}
Библиографические ссылки принято оформлять в виде [номер], в отличие от
ранее принятых [Автор, год] (см., напр., \cite{Trans}). 
Источник, процитированный выше, был набран командой
\begin{verbatim}
\bibitem{Trans}
\textit{Борисов~Г.~А., 
Тихомирова~Т.~А.}
Характеристики и свойства потерь 
энергии и мощности на пределах 
энергетического хозяйства региона //
Труды Карельского научного центра 
Российской академии наук. 2010. 
\No 3. С.~4--10.
\end{verbatim}

\section{Теоремоподобные окружения}
Для теорем, утверждений и пр. необходимо использовать соответствующие
окружения. Например:

\begin{State} 
В предложенной модели системы обслуживания при
$\rho=ES/ET<1$ условие $ES^{\alpha+1}<\infty$ является достаточным для 
конечности момента порядка $\alpha$ времени ожидания в системе, $ED^\alpha<\infty$.
\end{State}
\begin{proof}
Очевидно.
\end{proof}

В данном случае было использовано окружение \verb"\begin{State}...\end{State}". Для
набора доказательства использовалось окружение \verb"\begin{proof}...\end{proof}".
Доступные автору теоремоподобные окружения перечислены в Таблице~\ref{theorems}.

\begin{table}[H]
\centering
\caption{Теоремоподобные окружения}
\begin{tabular}{|l|l|}
\hline
\verb"Theorem" & Теорема\\
\verb"Lemma" & Лемма\\
\verb"State" & Утверждение\\
\verb"Corollary" & Следствие\\
\verb"Axiom" & Аксиома\\
\verb"Definition" & Определение\\
\verb"Example" & Пример\\
\verb"Remark" & Замечание\\
\hline
\end{tabular}
\label{theorems}
\end{table}

Для определений, примеров и замечаний используется прямое написание.
\begin{Example*}
Например, как в этом примере.
\end{Example*}

Соответствующие версии окружений <<со звездой>> также работают. Пример
выше был набран такой командой:

\begin{verbatim}
\begin{Example*}
Например, как в этом примере.
\end{Example*}
\end{verbatim}

\bfullwidth
\begin{table}[H]
\caption{Таблица, демонстрирующая возможность размещения на всю ширину страницы}
\begin{tabular}{|c|c|c|c|c|}
\hline
Первая колонка & вторая колонка & третья колонка & четвертая колонка & пятая колонна\\
\hline
\end{tabular}
\label{tab_width}
\end{table}
\efullwidth

\section{Рисунки и таблицы}
Рисунки и таблицы могут вставляться как на всю ширину страницы,
так и на ширину колонки. Желательно использовать рисунки формата pdf. 
Для конвертации из формата eps можно воспользоваться утилитой epstopdf.
Так, например, Рис.~\ref{fig1} был вставлен на ширину колонки командой
\begin{verbatim}
\begin{figure}[H]
\includegraphics[keepaspectratio=true,
 width=0.9\columnwidth]{delay_80.pdf}
\caption{Задержки в модели 
 10-узловой системы}
\label{fig1}
\end{figure}
\end{verbatim}

\begin{figure}[H]
    \includegraphics[keepaspectratio=true,width=0.9\columnwidth]{delay_80.pdf}
    \caption{Задержки в модели 10-узловой системы}
    \label{fig1}
\end{figure}



\section{Размещение на всю ширину страницы}
В стилевом файле предусмотрена возможность размещения формул, рисунков
и таблиц на ширину страницы. Для этого размещаемый элемент необходимо
заключить между командами \verb'\bfullwidth' и \verb'\efullwidth'.
 
Пример формулы на всю ширину страницы:
\bfullwidth
\begin{equation}
Y=A_1x+A_2x^2+\ldots +A_nx^n.
\end{equation}
\efullwidth
Набран пример следующим образом:
\begin{verbatim}
\bfullwidth
\begin{equation}
Y=A_1x+A_2x^2+\ldots +A_nx^n.
\end{equation}
\efullwidth
\end{verbatim}


Пример размещения таблицы на ширину страницы (см. таблицу~\ref{tab_width}):
\begin{verbatim}
\bfullwidth
\centering
\begin{table}[H]
\begin{tabular}{|c|c|c|c|c|}
\hline
Первая колонка & ...\\
\hline
\end{tabular}
\label{tab_width}
\end{table}
\efullwidth
\end{verbatim}

Рис.~\ref{fig5} демонстрирует возможности вставки по всей ширине страницы.
Это было достигнуто при помощи команды
\begin{verbatim}
\bfullwidth
\begin{figure}
\includegraphics[height=100mm,...}
\caption{Задержки в модели...}
\label{fig5}
\end{figure}
\efullwidth
\end{verbatim}
Следует обратить внимание на то, что вышеуказанная команда
вставит рисунок не ближе, чем на следующей странице сверху,
а не сразу на месте указания команды.

\section{Сведения об авторах}
После основного текста оформляются сведения об авторах. Используется
окружение \verb"\begin{aboutauthors}...\end{aboutauthors}". При этом следует обратить внимание, что
работа ведется в двухколоночном режиме, поэтому необходимо вручную 
указать разрыв колонки для отделения сведений на русском и английском
языках. Например:
\begin{verbatim}
\begin{aboutauthors}
\authorsname{Румянцев Александр...}
аспирант\\ 
...
\columnbreak
\authorsname{Rumyantsev, Alexander}
...
\end{aboutauthors}
\end{verbatim}

\section{Заключение}
Компиляцию исходного файла желательно выполнять с помощью макроса \verb"pdflatex".

В работе рассмотрены основные технические аспекты подготовки статьи
для сборника Трудов Карельского научного центра РАН. Предложения
и пожелания по доработке стилевого файла, а также текста этого документа
принимаются по электронному адресу, указанному в разделе <<Сведения об авторах>>.

\begin{thebibliography}{9}
%\bibitem[Борисов, Тихомирова(2010)]{Trans}
\bibitem{Trans}
\textit{Борисов~Г.~А., Тихомирова~Т.~А.} Характеристики и свойства потерь энергии
и мощности на пределах энергетического хозяйства региона //
Труды Карельского научного центра Российской академии наук. 2010. \No 3. С.~4--10.

\bibitem{Latexrus}
\textit{Львовский С. М.} Набор и верстка в системе \LaTeX. М., 2003. 448~с.

\bibitem{Averin 2006}
\textit{Аверин, А.Н.} Разработка сервиса поиска биграмм // Труды международной конференции «Корпусная лингвистика–2006. СПб., С.Петерб. ун-та., 2006.

\bibitem{epr:website}
\textit{Епрев, А. С.}Применение контекстных векторов в классификации текстовых документов. 2010. http://jre.cplire.ru/iso/oct10/1/text.html

\bibitem{Abney 1999}
\textit{Abney, S. and Light, M.}Hiding a semantic hierarchy in a markov model. In Proceedings of the Workshop on Unsupervised Learning in Natural Language Processing, ACL. 1999. 

\bibitem{Ciaramita 2000}
\textit{Ciaramita, M. and Johnson, M.}Explaining away ambiguity: Learning verb selectional preference with Bayesian networks. 2000. 

\bibitem{COTTRELL 1983}
\textit{COTTRELL, G. W. and SMALL, S. L.}A connectionist  scheme for modelling word  sense  disambiguation -- Cognition and brain theory. 1983. № 6. P. 89–120. 

\bibitem{LESK 1986}
\textit{LESK, M.}Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone --  Proceedings  of  the 5th SIGDOC. New York. 1986. P. 24–26.  

\bibitem{Miller 1990}
\textit{Miller, G.}Wordnet: An on-line lexical database. International Journal of Lexicography, 3(4). 1990.

\bibitem{Pedersen 1997}
\textit{Pedersen, T. and Bruce, R.}Distinguishing word senses in untagged text. Proc.EMNLP.Providence, RI, 1997.

\bibitem{Resnik 1997}
\textit{Resnik, P.}Selectional preference and sense disambiguation. In Proceedings of the ANLP-97 Workshop: Tagging Text with Lexical Semantics: Why, What, and How? 1997.

\bibitem{Savova 2005}
\textit{Savova, G.} Resolving ambiguities in biomedical text with unsupervised clustering approaches. University of Minnesota Supercomputing Institute Research Report, 2005.

\bibitem{VERONIS 1990}
\textit{VERONIS, J. and IDE, N.} Word  sense  disambiguation  with  very  large neural  networks  extracted  from machine readable dictionaries – Proceedings of the 13th International Conference on Computational Linguistics. Helsinki. 1990. P. 389–394.

\bibitem{SC:website}
\textit{SenseClusters}http://senseclusters.sourceforge.net

\bibitem{Schutze 1998}
\textit{Schutze, H.}Automatic Word Sense Discrimination. Computational Linguistics, vol. 24, number 1., 1998.

\bibitem{UMLS:website}
\textit UMLS Terminology Services (UTS). http://umlsks.nlm.nih.gov/kss/servlet/Turbine/template

\bibitem{WALTZ 1985}
\textit{WALTZ, D. L. and POLLACK, J. B.}Massively parallel parsing: a strongly interactive  model  of  natural  language interpretation – Cognitive science. 1985. № 9. P. 51–74.

\bibitem{Weeber 2001}
\textit{Weeber, M. and Mork, J. and Aronson, A.}Developing a test collection for biomedical word sense disambiguation. Proc. AMIA., 2001.





\end{thebibliography}
\end{articletext}


\section{СВЕДЕНИЯ ОБ АВТОРE:}

\begin{aboutauthors}
\authorsname{Кирилов Александр Николаевич}
доктор физико-математических наук\\ 
доцент\\
Институт прикладных математических исследований КарНЦ РАН\\ 
ул. Пушкинская, 11, Петрозаводск, Республика Карелия, Россия, 185910\\
эл. почта: kirillov@krc.karelia.ru\\
тел.: (8142) 766312

\columnbreak

\authorsname{Kirilov, Alexander}
Doctor (DSc) of Physics and Mathematics\\
Assistant Professor\\
Institute of Applied Mathematical Research, Karelian Research Centre, Russian Academy of Science\\
11 Pushkinskaya St., 185910 Petrozavodsk, Karelia, Russia\\
e-mail: kirillov@krc.karelia.ru\\
tel.: (8142) 766312
\end{aboutauthors}

\begin{aboutauthors}
\authorsname{Румянцев Александр Cергеевич}
аспирант\\ 
Институт прикладных математических исследований КарНЦ РАН\\ 
ул. Пушкинская, 11, Петрозаводск, Республика Карелия, Россия, 185910\\
эл. почта: ar0@krc.karelia.ru\\
тел.: (8142) 763370

\columnbreak

\authorsname{Rumyantsev, Alexander}
Institute of Applied Mathematical Research, Karelian Research Centre, Russian Academy of Science\\
11 Pushkinskaya St., 185910 Petrozavodsk, Karelia, Russia\\
e-mail: ar0@krc.karelia.ru\\
tel.: (8142) 763370
\end{aboutauthors}

\begin{aboutauthors}
\authorsname{Сихонина Ирина Александровна}
Математический факультет\\ 
Петрозаводский государственный университет\\
пр-кт Ленина, 33, Петрозаводск, Республика Карелия\\
тел.: +7(8142) 71-10-78\\
syawenka@mail.ru

\columnbreak

\authorsname{Sikhonina, Irina}
Student\\
Faculty of Mathematics\\
Petrozavodsk State University\\
Prospect Lenina, 33, Petrozavodsk, Republic of Karelia\\
tel.: +7(8142) 71-10-78\\
syawenka@mail.ru 
\end{aboutauthors}

\begin{aboutauthors}
\authorsname{Ткач Станислав Сергеевич}
Математический факультет\\ 
Петрозаводский государственный университет\\
пр-кт Ленина, 33, Петрозаводск, Республика Карелия\\
тел.: +7 (8142) 71-10-78\\
tkachkras@gmail.com

\columnbreak

\authorsname{Tkach, Stanislav}
Student\\
Faculty of Mathematics\\
Petrozavodsk State University\\
Prospect Lenina, 33, Petrozavodsk, Republic of Karelia\\
tel.: +7 (8142) 71-10-78\\
tkachkras@gmail.com 
\end{aboutauthors}

\begin{aboutauthors}
\authorsname{Ярышкина Екатерина Александровна}
Студентка\\ 
Математический факультет\\ 
Петрозаводский государственный университет\\
пр-кт Ленина, 33, Петрозаводск, Республика Карелия\\
+7 (8142) 71-10-78\\
kate.rysh@gmail.com

\columnbreak

\authorsname{Yaryshkina, Ekaterina}
Student\\
Faculty of Mathematics\\
Petrozavodsk State University\\
Prospect Lenina, 33, Petrozavodsk, Republic of Karelia\\
+7 (8142) 71-10-78\\
kate.rysh@gmail.com 
\end{aboutauthors}
\end{document}
