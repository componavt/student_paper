\section{Выявление значений слов из текста~ --- кластеризация посредством комитетов}
\begin{flushright}
\textit{Д. Ю. Янкевич} 
\end{flushright}

В статье \cite{Pantel 2002} представлен алгоритм автоматического обнаружения значений слов в тексте, названный \textit{кластеризация посредством комитетов} (Clustering By Committee (CBC)). Также авторы предлагают методологию оценки для автоматического измерения точности и полноты найденных значений.
\parindent=0,5cm

\textbf{Алгоритм} первоначально находит множество небольших кластеров, называемых комитетами, каждый из которых представляет собой одно из значений определяемого слова. Центр тяжести  членов комитета (мера связности с определяемым словом) используется в качестве вектора признаков кластера.
\parindent=0,5cm

Алгоритм CBC состоит из трех этапов.
\parindent=0,5cm

На этапе I для каждого элемента (слова) вычисляются $k$ наиболее похожих слов. Сначала весь список относящихся к слову значений сортируется по убыванию значений связи согласно формуле точечной взаимной информации (pointwise mutual information (PMI)~\cite{Manning 1999}), а затем, с помощью иерархического кластерного анализа по \textit{методу средней связи} \cite{Kim 1989}, вычисляется сходство между всеми элементами кластера попарно. Значение функции PMI~ \cite{Manning 1999} между предполагаемым значением слова (контекстом) и элементом (словом) вычисляется следующим образом: пусть $x$ --- это рассматриваемый элемент, а $y$ --- контекст. \textit{Точечная взаимная информация} между $x$ и $y$ определена как:

$$
pmi(x;y)\equiv log \frac{\displaystyle p(x,y)}{\displaystyle p(x)p(y)}=
$$
$$
=log\frac{\displaystyle p(x|y)}{\displaystyle p(x)}=log\frac{\displaystyle p(y|x)}{\displaystyle p(y)}.
$$
\parindent=0,5cm

При кластеризации посредством метода средних связей \textit{(average-link clustering)} вычисляется среднее сходство между данным объектом и всеми объектами в кластере, а затем, если найденное \textit{среднее значение сходства} достигает или превосходит некоторый заданный пороговый уровень сходства, объект присоединяется к этому кластеру \cite{Kim 1989}. Сложность этого алгоритма $O(n^2 \times  log (n))$, где $n$ --- число кластеризуемых элементов \cite{Jain 1999}.
\parindent=0,5cm

Благодаря оценке (PMI) на I этапе отдается предпочтение (строятся) большие и тесно связанные кластеры.
\parindent=0,5cm

На II этапе строится набор небольших кластеров, где элементы каждого кластера образуют комитет. В ходе работы <<Алгоритма~1>> формируется как можно больше комитетов при условии, что каждый вновь созданный комитет не слишком похож на любой из уже существующих комитетов. Если условие нарушается, комитет просто отбрасывается. Алгоритм описан ниже подробно.

\bfullwidth
\begin{algorithm}[H]
\SetAlgoLined
\KwData{$(E, S, \theta_1, \theta_2)$, где  $E$ - это список элементов, которые будут сгруппированы, база данных сходства $S$ (построена в ходе фазы I), пороги $\theta_1$ и $\theta_2$ (с помощью порога $\theta_1$ сохраняются только те кластеры, которые имеют значения, отличные от ранее обнаруженных, порог $\theta_2$ позволяет обнаружить элементы, не принадлежащие ни одному из кластеров)}
\KwResult{$C$ --- список комитетов}
\textbf{Step 1:}\\
\ForEach{$e \in E$}{\begin{enumerate}
\item Кластер $k$ наиболее <<близких>> (похожих) элементов $e$ из $S$ с помощью метода  \newline средней связи
\item Для каждого обнаруженного кластера $c$ вычислить следующую оценку:
$val = | C | \times avgsim(c)$, где $| C |$ --- количество элементов $c$ и $avgsim(c)$ --- усредненное сходство между всеми парами элементов кластера $c$.
\item Записать кластер с наивысшей оценкой в список $L$\end{enumerate}}
\textbf{Step 2:}\\
SortByDecreasingOrder$(c(val) \in L)$ // Сортировка кластеров в списке $L$ в порядке убывания их оценок $val$.\\
\textbf{Step 3:}\\
$C = \emptyset$ // Пусть  перечень комитетов $C$ будет изначально пустым.\\
\ForEach{$c \in L$}{// в отсортированном по убыванию порядке:
\begin{enumerate}
\item Вычислить центр тяжести, усредняя поэлементно значение векторов, и вычислить 
\newline вектор  PMI центроида (точно так же, как мы делали для отдельных элементов).
\item Если схожесть $c$ и центроида каждого комитета, ранее добавленного к $C$, ниже порогового $\theta_1$, то следует добавить $c$ в $C$.
\end{enumerate}}
\textbf{Step 4:}\\
\If{$C=\emptyset$}{\Return $C$}
\textbf{Step 5:}\\
$R = \emptyset$ // $R$ --- это множество остатков, то есть элементов, не охваченных ни одним из кластеров\\
\ForEach
{$e \in E$}
%{\ForEach
%{$c \in C$}
{\If{$sim(e,$ foreach  $c \in C) < \theta_2$ //  сходство по всем комитетам из $C$ меньше $\theta_2$ // }
{$R += e$ // то следует добавить $e$ в список остатков $R$.}
%}
}
\textbf{Step 6:}\\
\If{$R=\emptyset$}{\Return $C$ \Else{\Return $C \cup Algorithm1(R, S, \theta_1, \theta_2)$}}
\caption{II фаза кластеризации посредством комитетов (CBC)}
\label{alg}
\end{algorithm}
\efullwidth

В результате второго этапа построения CBC остаются кластеры, связанные более тесно (имеющие большее значение $val$, см. step 1 и 3 алгоритма~\ref{alg}).
\parindent=0,5cm

На заключительном III этапе работы алгоритма каждый элемент $e$ присваивается наиболее близкому кластеру следующим образом (при этом центроид членов комитета используется в качестве вектора характеристик кластера):

\bfullwidth
\begin{algorithm}[H]
\SetAlgoLined

\KwResult{Итоговые кластеры с максимальным значением связи ({\it val}) между словами}
Пусть $C$ - это список кластеров (изначально пустых).\\
Пусть $S$ - это первые 200 кластеров, наиболее похожие на $e$.\\
\While{$S$ не пуст}{пусть $c \in S$ наиболее близкий кластер к $e$\\
\If{сходство $(e, c) < \sigma$}{\textbf{конец цикла}}
\If{$c$ не схож ни с одним кластером в $C$}{присвоить $e$ к $c$;\\
удалить из $e$ его характеристики, которые перекрываются с характеристиками $c$;}
удалить $c$ из $S$
}
\caption{Присвоение элементов кластерам}
\label{alg1}
\end{algorithm}
\efullwidth

На III этапе кластер сохраняется только в случае, если его сходство со всеми ранее полученными кластерами ниже установленного порогового значения.
\parindent=0,5cm

Согласно дистрибутивной гипотезе (Distributional Hypothesis) \cite{Harris 1985} слова, употребляемые в сходных контекстах, близки по смыслу. Алгоритм \textit{Кластеризации посредством комитетов} \cite{Pantel 2002} разрешает лексическую многозначность, группируя слова согласно сходству их контекстов. Каждому полученному кластеру соответствует одно из значений слова.
\parindent=0,5cm

%В Табл.~\ref{yank} каждая запись показывает кластеры, которым принадлежит заглавное слово. Имена для кластеров Nq34, Nq137, $\ldots$ генерируются %автоматически. После каждого имени кластера находится число, обозначающее сходство между кластером и заглавным словом (т.е. рукав, сердце и одежда). %Далее перечисляются четыре слова, наиболее близкие центроиду кластера. Каждый кластер соответствует одному значению заглавного слова. Например, Nq34 %соответствует значению <<деталь одежды>>, а Nq137 соответствует значению <<ответвление русла реки>>.

\begin{table}[H]
\caption{Для построения кластеров использовались данные словарных статей Викисловаря: <<одежда>>,  <<рукав>> и  <<сердце>>}
\begin{tabular}{|c|c|m{5cm}|}
\hline
\multicolumn{3}{|c|}{Рукав}\\
\hline 
Nq34 & 0.39 & деталь, манжета, полотно\\
Nq137 & 0.20 & протока, русло, отмель, поток\\
Nq217 & 0.18 & шланг, труба, огнетушитель\\
\hline
\multicolumn{3}{|c|}{Сердце}\\
\hline
Nq72 &  0.27 & орган, костный мозг, почка\\
Nq866 & 0.17 & душа, рассудок, сознание\\
\hline
\multicolumn{3}{|c|}{Одежда}\\
\hline
Nq215 & 0.41 & мануфактура, юбка, брюки\\
Nq235 & 0.20 & покрытие, оболочка, дорога\\
\hline
\end{tabular}
\label{yank}
\end{table}

В Табл.~\ref{yank} каждая запись показывает кластеры, которым принадлежит заглавное слово. Имена для кластеров Nq34, Nq137, $\ldots$ генерируются автоматически. После каждого имени кластера находится число, обозначающее сходство между кластером и заглавным словом (т.е.  {\it рукав}, {\it сердце} и {\it одежда}). Далее перечисляются четыре слова, наиболее близкие центроиду кластера. Каждый кластер соответствует одному значению заглавного слова. Например, Nq34 соответствует значению <<деталь одежды>>, а Nq137 соответствует значению <<ответвление русла реки>>.


\textbf{Сравнение с алгоритмом UNICON.} CBC является разновидностью алгоритма UNICON \cite{Lin 2001}, который также строит центроид кластера, используя небольшой набор похожих элементов.
\parindent=0,5cm

Одним из основных различий между UNICON и CBC является то, что UNICON гарантирует, что различные комитеты не имеют одинаковых элементов, тем не менее, центры тяжести двух комитетов по-прежнему могут быть очень близкими (похожими). В~UNICON'е эта проблема решается объединением таких кластеров. В отличие от этого, на II этапе CBC создаются только те комитеты, центры тяжести которых отличны от всех ранее созданных комитетов.
\parindent=0,5cm

Есть разница и на III этапе CBC. Алгоритм UNICON плохо работает со словами, которые имеют несколько широко используемых (доминирующих) значений. Например, пусть значение <<отмычка>> является более употребимым для слова <<ключ>>, чем значение <<водный источник>>. Приведем смесь слов-синонимов к разным значениям слова <<ключ>>: пневмоключ, электроключ, родник, родничок, источник, криница, гидроключ, ключик, тангента, треншальтер, тумблер, знак, контролька, отпирка, виброплекс, шифр. В этом списке 10 значений относятся к значению <<отмычка()>>, 4 к <<водный источник()>> и 2 к значению <<знак()>>. По этому списку алгоритмом UNICON будут сгенерированы кластеры <<отмычка>>, <<шифрование>>, <<происхождение>>,<<криптография>>, <<кнопка>>, <<водный источник>>,<<переключатель>>, <<намек>>. Сходство между словом и полученными кластерами  является очень низким, к тому же есть кластеры, содержащие одинаковые слова. С другой стороны, CBC удаляет <<пересекающиеся>> (общие для двух кластеров) характеристики после того, как присвоит значение кластеру (допустим характеристики, относящиеся к значению <<отмычка>> слова <<ключ>> из вектора характеристик <<отмычка>>). В результате сходство между  кластером <<водный источник>> \{родник, родничок, источник, криница\} и пересмотренным вектором характеристик кластера <<водный источник>> становится намного выше. Что, в свою очередь, приводит к тому, что кластеры становятся гораздо точнее.
