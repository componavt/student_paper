\section{Бустинг}

\begin{flushright}
\textit{Т. В. Степкина, Ю. В. Чиркова}
\end{flushright} 

Бустинг --- это общий и доказуемо-эффективный метод получения очень точного правила предсказания путем комбинирования грубых и умеренно неточных эмпирических правил \cite{Freund 1999}. Метод бустинга разработан на основе модели обучения ``PAC'' (probably approximately correct learning).

Метод бустинга имеет множество реализаций. Работы, посвященные бустингу, обычно описывают какой-либо из его алгоритмов. Так, например, в работах \cite{Breiman, Marmanis} рассматривается  алгоритм arc-x4. В \cite{Freund 1997, Paclin} приводится алгоритм AdaBoost.M1. Мы рассмотрим бустинг на примере алгоритма AdaBoost, который является базовым для многих модификаций, а также имеет прочный теоретический фундамент и является результатом строгого вывода~\cite{Marmanis}.

Алгоритм AdaBoost был предложен в 1995 году Фройндом и Шапиро \cite{Freund 1996}. В нём исправлены многие недостатки предыдущих алгоритмов бустинга.

AdaBoost является адаптивным алгоритмом \cite{Freund 1999}, поскольку он может адаптироваться к уровням ошибок отдельных слабых гипотез. В названии ``Ada'' является сокращением от ``adaptive'' (адаптивный).

На вход алгоритма поступает обучающая выборка $(x_i;y_i);..;(x_m;y_m)$, где каждый элемент $x_i$ принадлежит некоторому домену или признаковому пространству $X$, и каждая метка $y_i$ принадлежит некоторому набору меток $Y$. Для каждого обучающего примера $i$ вес распределения для целых $t$ обозначается $D_t$ $(i)$, где $t$ --- это шаг алгоритма. За начальное распределение весов принимается $D_1(i)=~ 1/m$. Пусть метки принимают значения из множества  $Y=\{-1,1\}$. 

Далее на каждом шаге $t$, где $t = 1 \ldots T$, выполняется обучение с использованием текущего распределения $D_t$, после чего строится слабая гипотеза $h_t:~ X \to \{-1; 1\}$ с ошибкой первого рода $\epsilon_t=\Sigma_{\displaystyle (i:h_t (x_i)\ne y_i)}D_t(i)$, по которой выбирается уровень значимости $\alpha_t=\frac{\displaystyle 1}{\displaystyle 2}ln(\frac{\displaystyle 1-\epsilon_t}{\displaystyle \epsilon_t})$ и строится новое распределение для следующего шага
%\bfullwidth
%\begin{equation*}
$$
D_{t+1}(i)=\frac{D_{t^i}}{Z_t} \times 
\begin{cases}
e^{-\alpha t}\text{, если }h_t (x_i) = y_i \\
e^{\alpha t}\text{, если }h_t (x_i)\ne y_i
\end{cases}
=
$$
$$
= \frac{D_t (i)exp(-\alpha_t y_i h_t (x_i))}{Z_t}. 
$$
%\end{equation*}
%\efullwidth

Конечная гипотеза $H(x)$ ---  это среднее из большинства решений $T$ слабых гипотез, где $\alpha_t$ --- вес, присвоенный гипотезе $h_t$.

%\bfullwidth
%\begin{equation*}
$$
H(x)=sign(\sum_{t=1}^T\alpha_t h_t(x))
$$
%\end{equation*}
%\efullwidth

Идея алгоритма заключается в определении набора весов для обучающей выборки. Первоначально все веса примеров устанавливаются одинаково, но на каждом круге цикла веса неправильно классифицированных по гипотезе $h_t$ примеров увеличиваются. Таким образом получаются веса, которые относятся к сложным примерам.

Основное теоретическое свойство AdaBoost~ --- это способность алгоритма уменьшать ошибку обучения \cite{Freund 1999}. Фройнд и Шапиро показали, что, так как каждая слабая гипотеза немного лучше случайного выбора, ошибка обучения уменьшается с экспоненциальной скоростью.

В статье \cite{Freund 1999} показано, как ограничена ошибка обобщения конечной гипотезы в терминах ошибки обучения, размера выборки m, VC размерности (размерности Вапника --- Червоненкиса \cite{Schapire 1997}) пространства слабых гипотез и количества циклов Т. Также получена граница, не зависящая от Т. Это показывает, что бустинг AdaBoost не подвержен эффекту переобучения.

Так как ошибка обучения и ошибка обобщения ограничены, как показано в статье \cite{Freund 1999}, этот алгоритм действительно является бустинговым алгоритмом в том смысле, что он может эффективно преобразовать слабый алгоритм обучения в сильный, который может породить гипотезу со сколь угодно малой частотой ошибок, имея достаточное количество данных.

После того, как авторы рассмотрели бинарный случай, где целью является различие лишь между двумя возможными классами, они переходят к рассмотрению мультиклассного, более приближенного к реальности. Есть несколько способов приведения AdaBoost к мультиклассному случаю. Самое простое обобщение называется AdaBoost.M1 \cite{Freund 1997}, которое является приемлемым, если слабый обучаемый может достичь достаточно высокой точности на распределениях, созданных AdaBoost. Тем не менее, этот метод завершается неудачно, если слабый ученик не может достичь хотя бы 50\% точности при работе на этих  распределениях. Для такого случая было разработано несколько методов:

\begin{enumerate}
\item Методы, которые работают за счет преобразования мультиклассной задачи в большую бинарную задачу или в набор бинарных задач. Эти методы требуют дополнительных усилий в разработке слабого алгоритма обучения.
\item Технология, которая включает в себя метод Диттерича и Бакири --- метод выходных кодов, исправляющих ошибки \cite{Schapire 1997}.
\end{enumerate}

AdaBoost обладает определенными преимуществами. Его быстро и просто запрограммировать. Он не имеет никаких параметров для настройки, за исключением количества циклов. Он не требует никаких предварительных знаний о слабом обучаемом и поэтому может быть скомбинирован с любым методом для нахождения слабых гипотез.

Недостатки метода заключаются в следующем. Фактическая производительность бустинга на конкретной задаче явно зависит от данных и слабого обучаемого. Теоретически, бустинг может выполниться плохо, если данных недостаточно, слабые гипотезы слишком сложные, или наоборот слишком слабые. Также бустинг особенно восприимчив к шуму.

AdaBoost был протестирован эмпирическим путем многими исследователями. Например, Фройнд и Шапиро проверили AdaBoost на множестве эталонных наборов данных UCI \cite{Merz 1998} с использованием C4.5 \cite{Quinlan 1993} как слабого алгоритма обучения, а также алгоритм, который находит самое лучшее дерево решений с одним тестом. После проведения эксперимента был сделан вывод, что бустинг даже слабых деревьев решений с одним тестом, как правило, дает хорошие результаты, в то время как бустинг C4.5, как правило, дает алгоритм дерева принятия решений значительно улучшенной производительности.

Почти во всех этих экспериментах и для всех показателей эффективности бустинг работает так же хорошо или значительно лучше, чем в других методах испытаний. Бустинг также применяется к фильтрации текстов, проблемам ранжирования и проблемам классификации, возникающих при обработке естественного языка.
В работе \cite{Wu} бустинг наряду с другими семью WSD-методами, используется для решения тестовой задачи с китайской лексикой. Проведенные эксперименты показали, что бустинг по точности уступает только методу максимальной энтропии и классификатору, комбинирующему бустинг, наивный байесовский классификатор, метод максимальной энтропии и PCA-модель.
Для задачи с английской лексикой опыт применения бустинга описан в работах~\cite{esc1,esc2}. 
В них авторы рассматривают алгоритм LazyBoosting -- модификция AdaBoost.MH~\cite{schapire99}.
По результатам сравнительных экспериментов бустинг оказывается по точности лучше таких методов, 
как наивный байесовский классификатор, 
метод основанный на примерах (Exemplar Based) и MFS (naive Most-Frequent-Sense classifier).