\section{Сравнительные эксперименты в WSD: роль предпочтений в машинном обучении}

\begin{flushright}
\textit{Н. И. Коржицкий}
\end{flushright}

В работе Рэймонда Муни \cite{Mooney 1996} представлено одно из первых сравнений разных по природе методов WSD на одних и тех же данных. В статье \cite{Mooney 1996} проведена серия экспериментов, в которых сравнивалась способность различных обучающихся алгоритмов определять значение слова в зависимости от контекста. 

В машинном обучении под термином \textit{bias} (пристрастие, тенденция, предпочтение)  понимается любое основание для предпочтения одного обобщения другому~\cite{Mooney 1996}. В деревьях принятия решений предпочтение (\textit{bias}) отдается простым деревьям решений, в нейронных сетях -- линейным пороговым функциям, а в байесовском классификаторе -- функциям, учитывающим условную независимость свойств. Чем лучше <<предпочтение>> обучающегося алгоритма соответствует характеристикам конкретной задачи, тем лучше будет результат. Большинство обучающихся алгоритмов обладают <<предпочтением>> наподобие Бритвы Оккама. В таких алгоритмах выбираются гипотезы, которые могут быть представлены меньшим количеством информации на каком-нибудь языке представлений. Однако компактность, с которой (деревья решений, дизъюнктивная нормальная форма, сети с линейным пороговым значением) представляют конкретные функции -- может существенно различаться. Поэтому различные <<предпочтительные>> оценки могут работать лучше или хуже в конкретных задачах. Одной из основных целей в машинном обучении является поиск <<предпочтений>> с целью решения прикладных практических задач.

Выбор правильного <<предпочтения>> и обучающегося алгоритма является сложной задачей. Простым подходом является автоматизация выбора метода на основе результатов внутренней перекрестной валидации. 
Другой подход, который называется мета-обучением (meta-learning), заключается в том, чтобы сформировать набор правил (или аналогичный классификатор), который на основании предметных  признаков, описывающих задачу, предсказывал бы, когда обучающийся алгоритм будет срабатывать наилучшим образом.

Описанный в \cite{Mooney 1996} эксперимент заключается в определении значения английского слова \textit{line} среди 6 возможных вариантов (\textit{строка, ряд, дивизия, телефон, веревка, продуктовая линия}). Данные для проведения экспериментов взяты из работы \cite{Leacock 1993}.  

Для получения обучающей выборки брались предложения со словом \textit{line}, и им в соответствие ставилось одно из 6 значений. Распределение значений неравномерно: включение в список источников журнала The Wall Street Journal привело к тому, что одно из значений встречалось в 5 раз чаще всех остальных \cite{Leacock 1993}.

\begin{table*}
\centering
\caption{Шесть значений слова \textit{line} из Английского Викисловаря и Русского Викисловаря}
\begin{tabular}{|m{2cm}|m{2cm}|m{5cm}|m{5cm}|}
%\begin{tabular}{|m{2cm}|m{2cm}|m{6cm}|  C{4cm} |}

\hline
\centering \textbf{Ключевое слово} & \centering \textbf{Перевод} & \centering \textbf{Толкование \mbox{на английском}} (Английский~Викисловарь) & \centering \textbf{Толкование на русском} (Русский Викисловарь) \tabularnewline

\hline
Тext & Строка & A small amount of text & Ряд слов, букв или иных знаков, написанных или напечатанных в одну линию \\
\hline
Formation & Ряд & A more-or-less straight sequence of people, objects, etc. & Несколько объектов, расположенных в линию или следующих один за другим\\
\hline
Division & Дивизия & A formation, usually made up of two or three brigades & Тактическое воинское соединение \\
\hline
Phone & Телефон & The wire connecting one telegraphic station with another, a telephone or internet cable & То же, что телефонный \mbox{номер} \\
\hline
Cord & Веревка & A rope, cord, string, or thread, of any thickness  & Гибкое и длинное 
изделие,~-- чаще всего сплетенное или свитое из льняных (или пеньковых, 
полимерных и~т.~п.) волокон или прядей \\
\hline
Product & Продукто\-вая линия & The products or services sold by a business, or by extension, the business itself & Совокупность однородной продукции единого назначения  \\
\hline
\end{tabular}
\label{tbl5}
\end{table*}

В работе \cite{Charles 1993} было установлено, что наиболее эффективными при решении задачи WSD являются алгоритмы на основе дерева решений (decision tree). Данный класс методов обходил по точности и скорости работы класс нейронных сетей. Другие исследования \cite{Mooney 1995} показали, что класс методов индуктивного логического программирования (inductive logic programming) справляется с задачей разрешения лексической многозначности слова лучше алгоритмов на основе дерева решений. 

В серии экспериментов в \cite{Mooney 1996} сравнивались следующие методы: байесовский классификатор, перцептрон, C4.5, метод k-ближайших соседей и модификации алгоритма FOIL: PFOIL-DLIST, PFOIL-DNF, PFOIL-CNF. 

После проведения сравнительных экспериментов, заключавшихся в обучении и определении значения слова \textit{line,} было выяснено, что байесовский классификатор и перцептрон работают точнее других рассмотренных методов. 

Эксперименты проводились с разными размерами обучающей выборки для того, чтобы выяснить, какого рода зависимость имеет место между точностью определения значения и размером выборки. На рис.~\ref{kor1} отображена зависимость точности работы алгоритмов от размера выборки. При увеличении размера обучающей выборки сначала происходит резкий рост точности, последующий прирост точности становится незначительным.

Эксперименты учитывали не только точность определения значения, но и требовательность алгоритма к ресурсам в процессе обучения и работы. На рис.~\ref{kor2} можно увидеть зависимость времени обучения от размера выборки. Самыми быстрообучаемыми оказались байесовский классификатор и перцептрон, а самыми медленными~-- нормальные формы (рис.~\ref{kor2}).

\begin{figure*}
        \centering
        \begin{subfigure}[b]{0.6\textwidth}
                \includegraphics[keepaspectratio=true, width=0.9\columnwidth]{line_wsd_2_time.png}
                \caption{Зависимость времени, затраченного на обучение \mbox{алгоритмов}, от размера обучающей выборки}
                \label{kor2}
        \end{subfigure}%
        \quad% add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
             % (or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.6\textwidth}
                \includegraphics[keepaspectratio=true, width=0.9\columnwidth]{line_wsd_3_testing_time.png}
                \caption{Зависимость времени работы алгоритмов от размера обучаю\-щей выборки}
                \label{kor3}
        \end{subfigure}%
        \quad% add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
             % (or a blank line to force the subfigure onto a new line)
             
                \begin{subfigure}[b]{0.6\textwidth}
                \includegraphics[keepaspectratio=true, width=0.9\columnwidth]{line_wsd_1_accuracy.png}
                \caption{Рост точности решения WSD-задачи для разных \mbox{алгоритмов при увеличении размера обучающей выборки}}
                \label{kor1}
        \end{subfigure}
        \caption{Сравнение времени обучения, времени работы и результатов работы алгоритмов PFOIL-DLIST, PFOIL-DNF, PFOIL-CNF, C4.5, Naive Bayes -- наивный баейсовский классификатор; Perceptron -- персептрон; 3 Nearest Neighbor -- метод 3-х ближайших соседей при определении значения слова \textit{line}~\cite{Mooney 1996}}\label{figure_alg_comparison}
\end{figure*}

На рис.~\ref{kor3} представлена зависимость времени работы алгоритмов от размера обучающей выборки. Время работы алгоритмов дает другую картину: байесовский классификатор и перцептрон работают долго при максимальном размере обучающей выборки, в то время как остальные методы решают WSD задачу за постоянное время (рис.~\ref{kor3}). 