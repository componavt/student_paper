%\bfullwidth
\section{WSD на основе нейронных сетей, построенных по данным машиночитаемых словарей}
%\efullwidth

\begin{flushright}
\textit{А. Н. Кириллов}
\end{flushright}

\parindent=0,5cm

Использование нейронных сетей (NN)  для WSD было предложено в 80-е гг. в работах \cite{COTTRELL 1983,WALTZ 1985}. В типичной NN на вход   подается   слово, значение которого требуется установить, то есть целевое (\textit{target}) слово, а также --- контекст (фраза) его содержащий. Узлы выхода соответствуют различным значениям слова. В процессе обучения, когда значение тренировочного целевого слова известно, веса связующих узлы соединений (связей) настраиваются таким образом, чтобы по окончании обучения выходной узел, соответствующий истинному значению целевого слова, имел наибольшую активность. Веса соединений могут быть  положительными или отрицательными, и настраиваются посредством рекуррентных алгоритмов (алгоритм обратного распространения ошибки,  рекуррентный метод наименьших квадратов и так далее). Сеть может содержать скрытые (hidden) слои, состоящие из узлов, соединенных как прямыми, так и обратными связями. Для представления входной информации обычно используется одна из двух схем:  распределенная (distributed) или локалистская (localist ) (\cite{Azzini},  \cite{COTTRELL 1989}, \cite{Hinton 1986} ). 

В работе \cite{VERONIS 1990} описан метод автоматического построения \textbf{\textit{очень больших нейронных сетей}} (VLNN) с помощью текстов, извлекаемых из машиночитаемых словарей (MRD), и рассмотрено использование этих сетей в задачах разрешения лексической неоднозначности. Поясним основную идею VLNN.  Широко известен метод Леска \cite{LESK 1986}  использования  информации  из MRD для  задачи  WSD.  Суть этого метода состоит в вычислении  так называемой \textit{степени пересечения}, то есть  количества общих слов в словарных определениях слов из контекста (<<окна>>) условного размера, содержащего целевое слово. Основной недостаток метода Леска --- зависимость  от  словарной  статьи, то есть от слов, входящих в нее. Стратегия преодоления  этого  недостатка --- использование словарных статей, определяющих слова, входящие в другие словарные статьи, начиная со словарных статей, соответствующих  словам  из  контекста. Таким образом, образуются  достаточно  длинные   пути  из слов, входящих в словарные статьи. Эта  идея  лежит  в  основе  топологии VgN. В работе \cite{VERONIS 1990}  для построения  VLNN использован  словарь  Collins  English  Dictionary.

\textbf{Топология сети.} Целевое слово представлено  узлом, соединенным активирующими  связями со смысловыми узлами, представляющими все возможные значения слова, имеющиеся  в словарных статьях. Каждый смысловой узел, в свою очередь, соединен  активирующими  связями с узлами, представляющими  слова в словарной статье, соответствующей толкованию данного значения. Процесс соединения повторяется многократно, создавая сверхбольшую сеть взаимосвязанных узлов. В идеале сеть может содержать весь словарь. Авторы, по практическим соображениям, ограничиваются несколькими тысячами узлов и 10–20 тысячами соединений. Слова представлены своими леммами (каноническими формами). Узлы, представляющие различные значения данного слова, соединены запрещающими (inhibitory) связями.  

\textbf{Алгоритм.} При запуске сети первыми активируются узлы входного  слова  (согласно принятой кодировке). Затем  каждый  входной узел посылает активирующий сигнал своим смысловым узлам, с которыми он соединен. В результате  сигналы распространяются по всей сети в течение определенного числа циклов. В каждом цикле узлы слова и  его значений получают обратные сигналы от узлов, соединенных с ними. Узлы конкурирующих значений посылают взаимно подавляющие сигналы.  Взаимодействие  сигналов  обратной  связи  и  подавления,  в соответствии со стратегией <<победитель получает все>>, позволяет увеличить активацию узлов-слов и соответствующих им правильных узлов-значений,  одновременно  уменьшая  активацию  узлов,  соответствующих неправильным значениям. После нескольких десятков  циклов сеть стабилизируется  в  состоянии, в котором активированы  только узлы-значения с наиболее активированными связями  с    узлами-словами. При обучении сети  используется  метод  обратного  распространения (\textit{back  propagation}).
ется следующая предобработка текста: удаляются знаки препинания, все слова переводятся в нижний регистр, все слова приводятся к их начальной форме (лемматизация). 
В \cite{Pedersen 2000} контексты делятся на два окна: левое и правое. В первое попадают слова, встречающиеся слева от неоднозначного слова, и, соответственно, во второе --- встречающиеся справа.
\parindent=0,5cm

Окна контекстов могут принимать 9 различных размеров: 0, 1, 2, 3, 4, 5, 10, 25 и 50 слов. Первым шагом в ансамблевом подходе является обучение отдельных наивных байесовских классификаторов для каждого из 81 возможных сочетаний левого и правого размеров окон. В статье  \cite{Pedersen 2000} наивный байесовский классификатор $(l,r)$ включает в себя $l$ слов слева от неоднозначного слова и $r$ слов справа. Исключением является классификатор (0,0), который не включает в себя слов ни слева, ни справа. В случае нулевого контекста классификатору присваивается \textbf{априорная вероятность} многозначного слова (равная  вероятности  встретить наиболее употребимое значение).
\parindent=0,5cm

Следующий шаг в \cite{Pedersen 2000} при построении ансамбля --- это выбор классификаторов, которые станут членами ансамбля. 81 классификатор группируется в три общие категории, по размеру окна контекста. Используются три таких диапазона: узкий (окна шириной в 0, 1 и 2 слова), средний (3, 4, 5 слов), широкий (10, 25, 50 слов). Всего есть 9 возможных комбинаций, поскольку левое и правое окна отделены друг от друга. Например, наивный байес (1,3) относится к диапазону категории (узкий, средний) поскольку он основан на окне из одного слова слева и окне из трех слов справа.  Наиболее точный классификатор в каждой из 9 категорий диапазонов выбирается для включения в ансамбль. Затем каждый из 9 членов классификаторов голосует за наиболее вероятное значение слова с учетом контекста. После этого ансамбль разрешает многозначность путем присвоения целевому слову значения, получившего наибольшее число голосов. 
\parindent=0,5cm

\textbf{Экспериментальные данные.} Для экспериментов были выбраны английские слова \textit{line} и \textit{interest.} Источником статистических данных по этим словам послужили работы \cite{Leacock 1993}, \cite{Bruce 1994}. В статье приводится информация о частоте использования шести значений для каждого из этих слов (Табл.~\ref{tbl3_kir}, Табл.~\ref{tbl4_kir}). 

\begin{table}[H]
\centering
\caption{Число употреблений слова \textit{line} (столбец \textit{count}) для шести наиболее часто встречаемых значений (значения из те\-за\-уру\-са WordNet, столбец \textit{sense}) по данным корпусов \textit{ACL/DCI Wall Street Journal и American Printing House for the Blind}}
\begin{tabular}{| m{5.5cm} |c |}
\hline
{\bf Значение} &{\bf Частота}\\ 
\hline
product & 2218\\  \hline
written or spoken text & 405\\  \hline
telephone connection & 429\\  \hline
formation of people or things;  queue & 349\\ \hline   
an artificial division; boundary & 376\\   \hline
a thin, flexible object; cord & 371\\
\hline
{\bf Всего} & 4148\\
\hline
\end{tabular}
\label{tbl3_kir}
\end{table}

\begin{table}[H]
\centering
\caption{Число употреблений слова \textit{interest} (столбец \textit{count}) для шести наиболее часто встречаемых значений (значения из словаря Longman Dictionary of Contemporary English, столбец \textit{sense}). Этот набор данных был получен в 1994 году Брюсом и Виебе \cite{Bruce 1994} путем указания значений для всех вхождений слова \textit{interest} в корпус ACL/DCI Wall Street Journal}
\begin{tabular}{| m{5.5cm} | c |}
\hline
{\bf Значение} &{\bf Частота}\\ 
\hline
money paid for the use of money & 1252\\ \hline
a share in a company or business & 500\\ \hline
readiness to give attention & 361\\ \hline
advantage, advancement or favor & 178\\ \hline
activity that one gives attention to & 66\\ \hline
causing attention to be given to & 11\\
\hline
{\bf Всего} & 2368\\
\hline
\end{tabular}
\label{tbl4_kir}
\end{table}


\parindent=0,5cm
\textbf{Результаты экспериментов.} Итогом проделанной работы стали обучение и проверка 81 наивного байесовского классификатора на многозначных словах \textit{line} и \textit{interest.} Точность разрешения лексической многозначности составила 89\% для слова \textit{interest} и 88\% для слова \textit{line.} В \cite{Pedersen 2000} было получено, что ансамбль классификаторов с голосованием простым большинством дает более высокую точность, чем взвешенное голосование. Например, для слова \textit{interest} при голосовании простым большинством точность составила 89\%, а взвешенное голосование дало только 83\%.
