\section{Построение сочетаемостных ограничений на основе байесовских сетей для разрешения многозначности}
\begin{flushright}
\textit{И. А. Сихонина} 
\end{flushright}

В статье \cite{Ciaramita 2000} представлена байесовская модель, применяемая для разрешения лексической многозначности глаголов. Авторы рассматривают такое понятие, как сочетаемостные ограничения (selectional preferences). \textit{Сочетаемостные ограничения} (далее SP) --- это закономерности использования глагола относительно семантического класса его параметров (субъект, объект (прямое дополнение) и косвенное дополнение).
\parindent=0,5cm

Модели автоматического построения SP важны сами по себе и имеют приложения в обработке естественного языка. Сочетаемостные ограничения глагола могут применяться для получения возможных значений неизвестного параметра при известных глаголах; например, из предложения \textit{<<Осенние хххх жужжали и бились на стекле>>} легко определить, что “xxxx” --- мухи. При построении предложения SP позволяют отранжировать варианты и выбрать лучший среди них. Исследование SP могло бы помочь в понимании структуры ментального лексикона. 
\parindent=0,5cm

Системы обучения SP без учителя обычно комбинируют статистические подходы и подходы, основанные на знаниях. Компонент базы знаний (здесь WordNet \cite{Miller 1990}) --- это обычно база данных, в которой слова сгруппированы в классы. 
\parindent=0,5cm

Статистический компонент состоит из пар предикат-аргумент, извлеченных из неразмеченного корпуса. В тривиальном алгоритме можно было бы получить список слов (прямых дополнений глагола), и для тех слов, которые есть в WordNet, вывести их семантические классы. В работе \cite{Ciaramita 2000} семантическим классом называется \textit{синсет} (от англ. \textit{\textbf{syn}onym \textbf{set}}, группа синонимов) тезауруса WordNet, то есть класс соответствует одному из значений слова. Таким образом, в тривиальном алгоритме на основе данных WordNet можно выбрать классы (значения слов), с которыми употребляются (встречаются в корпусе) глаголы.
\parindent=0,5cm

Например, если в исходном корпусе текстов глагол \textit{ползать} употребляется со словом \textit{ящерица}, принадлежащим классу РЕПТИЛИИ, то в модели построения SP будет записано, что <<глагол \textit{ползать} употребляется со словами из класса РЕПТИЛИИ>>. Если слово \textit{крокодил}, во-первых, также встречается в тексте с глаголом \textit{ползать}, во-вторых, слово \textit{крокодил} принадлежит сразу двум классам: РЕПТИЛИЯ и ВЕРТОЛЁТ, то из этого следует, что модель SP будет расширена информацией о том, что «глагол \textit{ползать} употребляется со словами из классов и РЕПТИЛИЯ, и ВЕРТОЛЁТ».
\parindent=0,5cm

В ранее разработанных моделях (Резник (1997) \cite{Resnik 1997}, Абни и Лайт (1999) \cite{Abney 1999}) было обнаружено, что главная трудность в таком тривиальном алгоритме --- это наличие неоднозначных слов в обучающих данных. В тех же работах (\cite{Resnik 1997}, \cite{Abney 1999}) были предложены более сложные модели, в которых предполагается, что все значения многозначных слов появляются с одинаковой частотой. 
\parindent=0,5cm

\textbf{Байесовские сети} или байесовские сети доверия (БСД) состоят из множества переменных (вершин) и множества ориентированных ребер, соединяющих эти переменные. Такой сети соответствует ориентированный ациклический граф. Каждая переменная может принимать одно из конечного числа взаимоисключающих состояний. Пусть все переменные будут бинарного типа, то есть принимают одно из двух значений: истина или ложь. Любой переменной \textit{А} с родителями $B_1,... , B_n$ соответствует таблица условных вероятностей (conditional probability table, далее CPT).
\parindent=0,5cm

Например, построим SP для глагола \textit{ползать} и сеть на Рис.~\ref{ris1} будет базой знаний.
\begin{figure}[H]
    \includegraphics[keepaspectratio=true,width=0.9\columnwidth]{bayesian_network_crocodile.jpg}
    \caption{Байесовская сеть для многозначного существительного \textit{крокодил}}
    \label{ris1}
\end{figure}
\parindent=0,5cm

Глагол \textit{ползать} употребляется со словами \textit{крокодил} и \textit{ящерица}. Переменные ВЕРТОЛЁТ и РЕПТИЛИЯ соответствуют более общим абстрактным значениям, переменные \textit{крокодил} и \textit{ящерица} являются более узкими, конкретными значениями. Переменная РЕПТИЛИЯ может принимать одно из двух значений, соответствующих словам \textit{крокодил} и \textit{ящерица}, именно эту задачу определения значения и нужно решить.

\begin{table}[H]
\centering
\caption{Условные вероятности переменных \textit{крокодил} и \textit{ящерица} в зависимости от значений переменных ВЕРТОЛЁТ и РЕПТИЛИЯ, где (В, Р, к, я --- это аббревиатуры слов ВЕРТОЛЁТ, РЕПТИЛИЯ, \textit{крокодил} и \textit{ящерица})}
\begin{tabular}{|c|c|c|c|c|}
\hline
%" " & \multicolumn{4}{|c|}{ \textit{Р}(\textit{X = x}$\vert$\textit{Y_1 = y_1, Y_2 = y_2})} \\
" " & \multicolumn{4}{|c|}{ $P(X=x|Y_1=y_1,Y_2=y_2)$ } \\
\hline
" " & В,Р & В,\textlnot Р & \textlnot В,Р & \textlnot В, \textlnot Р\\
\hline
к = \textit{true} & 0,99 & 0,99 & 0,99 & 0,01\\
к = \textit{false} & 0,01 & 0,01 & 0,01 & 0,99\\
\hline
я = \textit{true} & 0,99 & 0,99 & 0,01 & 0,01\\
я = \textit{false} & 0,01 & 0,01 & 0,99 & 0,99\\
\hline
\end{tabular}
\label{tbl1}
\end{table}

\parindent=0,5cm
При построении Табл.~\ref{tbl1} условных вероятностей (CPT), учтем следующие предположения:

\begin{itemize}
\item вероятность, что выбираем какой-либо из концептов (ВЕРТОЛЁТ и РЕПТИЛИЯ) очень мала, то есть \textit{P}(\textit{В=true}) = \textit{P}(\textit{Р=true}) = 0,01, следовательно, велика вероятность, что концепты не выбраны: \textit{P}(\textit{В=false}) = \textit{P}(\textit{Р= false}) = 0,99;
\item если какой-либо из концептов истинен (В, Р), то <<выпадает>> слово \textit{крокодил};
\item если концепт РЕПТИЛИЯ истинен, то растут шансы встретить слово \textit{ящерица};
\end{itemize}
\parindent=0,5cm

Из Табл.~\ref{tbl1} вероятности появления слов следует вывод, что использование разу двух значений слова \textit{крокодил (рептилия и вертолёт МИ-24)} маловероятно. Вероятность использования значения РЕПТИЛИЯ намного больше чем значения ВЕРТОЛЁТ. Таким образом гипотеза <<вертолёт>> <<отброшена>> (“explaining away”).
\parindent=0,5cm

\textbf{Байесовские сети для построения SP.} 
Иерархия существительных в WordNet представлена в виде ориентированного ациклического графа. 
Синсет узла принимает значение <<истина>>, если глагол <<выбирает>> существительное из набора синонимов. 
Априорные вероятности задаются на основе двух предположений: во-первых, маловероятно, 
что глагол будет употребляться только со словами какого-то конкретного синсета, 
и во-вторых, если глагол действительно употребляется только со словами из данного синсета (например, синсет ЕДА), 
тогда должно быть правомерным употребление этого глагола с гипонимами этого синсета (например, ФРУКТ).
\parindent=0,5cm

Те же предположения (что для синсетов) верны и для употреблений слов с глаголами:
\begin{enumerate}
\item слово, вероятно, является аргументом глагола в том случае, если глагол употребляется с каким-либо из значений этого слова;
\item отсутствие связки глагол-синсет говорит о малой вероятности того, что слова этого синсета употребляются с глаголом.
\end{enumerate}
\parindent=0,5cm

Словам <<вероятно>> и <<маловероятно>> должны быть приписаны такие числа, сумма которых равна единице. 
\parindent=0,5cm

Находкой работы \cite{Ciaramita 2000} является разъяснение стратегии “explaining away”, то есть отбрасывание маловероятных значений слов при построении сочетаемостных ограничений. Такая стратегия является неотъемлемым свойством байесовских сетей и байесовского вывода, полезным свойством при разрешении лексической многозначности. 
