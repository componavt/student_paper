\section{Разрешение лексической многозначности методом ансамбля байесовских классификаторов}

\begin{flushright}
\textit{А. Л. Чухарев, Т. В. Каушинис}
\end{flushright}

В работе Педерсена \cite{Pedersen 2000} рассматривается подход к разрешению лексической многозначности слов (WSD), подразумевающий создание ансамбля наивных байесовских классификаторов, каждый из которых основан на оценке вероятности вхождения определенных слов в контекст целевого слова, значение которого определяется.
\parindent=0,5cm

При разрешении лексической многозначности, представленном в виде задачи обучения с учителем, применяют статистические методы и методы машинного обучения к размеченному корпусу. В таких методах словам корпуса, для которых указано значение, соответствует набор языковых свойств. Педерсен \cite{Pedersen 2000} относит к языковым свойствам два вида особенностей: так называемые простые лексические признаки (shallow lexical features) и более сложные лингвистически обусловленные признаки (lingvistically motivated features). К первым относятся совместная встречаемость слов (co-occurence) и словосочетания (collocations), в то время как вторые включают в себя такие свойства, как часть речи и отношение действие-объект. Обычно алгоритмы обучения строят модели классификаторов значений по этим языковым свойствам.

Автор статьи \cite{Pedersen 2000} предлагает подход, основанный на объединении ряда простых классификаторов в ансамбль, который разрешает многозначность с помощью голосования простым большинством голосов. Педерсен утверждает \cite{Pedersen 2000}, что, во-первых, более сложные алгоритмы обычно не улучшают точность разрешения. Во-вторых, совместная встречаемость слов и словосочетаний имеют большее влияние на точность разрешения, чем оперирование более сложной лингвистической информацией.
\parindent=0,5cm


В рассматриваемой статье \cite{Pedersen 2000} в ансамбль объединяются наивные байесовские классификаторы. При таком подходе предполагается, что все переменные, участвующие в представлении проблемы, --- условно независимы при фиксированном значении переменной классификации. В проблеме разрешения лексической многозначности существует понятие контекста, в котором встречается многозначное слово. Этот контекст представляется в виде функции переменных $(F_1, F_2, \ldots , F_n)$, а значение многозначного слова представлено в виде классификационной переменной (S). Все переменные бинарные. Переменная, соответствующая слову из контекста, принимает значение ИСТИНА, если это слово находится на расстоянии определенного количества слов слева или справа от целевого слова. Совместная вероятность наблюдения определенной комбинации переменных контекста с конкретным значением слова выражается следующим образом: 
\\
\\
$p(F_1, F_2, \ldots , F_nS) = p(S)\Pi^n_{i=1}p(F_i \vee S)$ 
\\
\\
где $p(S)$ и $p(F_i|S)$ --- параметры данной модели. Для оценки параметров достаточно знать частоты событий, описываемых взаимозависимыми переменными $(F_i,S)$. Эти значения соответствуют числу предложений, где слово, представляемое $F_i$, встречается в некотором контексте многозначного слова, упомянутого в значении $S$. Если возникают нулевые значения параметров, то они сглаживаются путем присвоения им по умолчанию очень маленького значения. После оценки всех параметров модель считается  обученной и может быть использована в качестве классификатора.
\parindent=0,5cm

Контекст в \cite{Pedersen 2000} представлен в виде bag-of-words (модель «мешка слов»). В этой модели выполняется следующая предобработка текста: удаляются знаки препинания, все слова переводятся в нижний регистр, все слова приводятся к их начальной форме (лемматизация). 
В \cite{Pedersen 2000} контексты делятся на два окна: левое и правое. В первое попадают слова, встречающиеся слева от неоднозначного слова, и, соответственно, во второе --- встречающиеся справа.
\parindent=0,5cm

Окна контекстов могут принимать 9 различных размеров: 0, 1, 2, 3, 4, 5, 10, 25 и 50 слов. Первым шагом в ансамблевом подходе является обучение отдельных наивных байесовских классификаторов для каждого из 81 возможных сочетаний левого и правого размеров окон. В статье  \cite{Pedersen 2000} наивный байесовский классификатор $(l,r)$ включает в себя $l$ слов слева от неоднозначного слова и $r$ слов справа. Исключением является классификатор (0,0), который не включает в себя слов ни слева, ни справа. В случае нулевого контекста классификатору присваивается \textbf{априорная вероятность} многозначного слова (равная  вероятности  встретить наиболее употребимое значение).
\parindent=0,5cm

Следующий шаг в \cite{Pedersen 2000} при построении ансамбля --- это выбор классификаторов, которые станут членами ансамбля. 81 классификатор группируется в три общие категории, по размеру окна контекста. Используются три таких диапазона: узкий (окна шириной в 0, 1 и 2 слова), средний (3, 4, 5 слов), широкий (10, 25, 50 слов). Всего есть 9 возможных комбинаций, поскольку левое и правое окна отделены друг от друга. Например, наивный байес (1,3) относится к диапазону категории (узкий, средний) поскольку он основан на окне из одного слова слева и окне из трех слов справа.  Наиболее точный классификатор в каждой из 9 категорий диапазонов выбирается для включения в ансамбль. Затем каждый из 9 членов классификаторов голосует за наиболее вероятное значение слова с учетом контекста. После этого ансамбль разрешает многозначность путем присвоения целевому слову значения, получившего наибольшее число голосов. 
\parindent=0,5cm

\textbf{Экспериментальные данные.} Для экспериментов были выбраны английские слова \textit{line} и \textit{interest.} Источником статистических данных по этим словам послужили работы \cite{Leacock 1993}, \cite{Bruce 1994}. В статье приводится информация о частоте использования шести значений для каждого из этих слов (Табл.~\ref{tbl3_chu}, Табл.~\ref{tbl4_chu}). 

\begin{table}[H]
\centering
\caption{Число употреблений слова \textit{line} (столбец \textit{count}) для шести наиболее часто встречаемых значений (значения из те\-за\-уру\-са WordNet, столбец \textit{sense}) по данным корпусов \textit{ACL/DCI Wall Street Journal и American Printing House for the Blind}}
\begin{tabular}{| m{5.5cm} |c |}
\hline
{\bf Значение} &{\bf Частота}\\ 
\hline
product & 2218\\  \hline
written or spoken text & 405\\  \hline
telephone connection & 429\\  \hline
formation of people or things;  queue & 349\\ \hline   
an artificial division; boundary & 376\\   \hline
a thin, flexible object; cord & 371\\
\hline
{\bf Всего} & 4148\\
\hline
\end{tabular}
\label{tbl3_chu}
\end{table}

\begin{table}[H]
\centering
\caption{Число употреблений слова \textit{interest} (столбец \textit{count}) для шести наиболее часто встречаемых значений (значения из словаря Longman Dictionary of Contemporary English, столбец \textit{sense}). Этот набор данных был получен в 1994 году Брюсом и Виебе \cite{Bruce 1994} путем указания значений для всех вхождений слова \textit{interest} в корпус ACL/DCI Wall Street Journal}
\begin{tabular}{| m{5.5cm} | c |}
\hline
{\bf Значение} &{\bf Частота}\\ 
\hline
money paid for the use of money & 1252\\ \hline
a share in a company or business & 500\\ \hline
readiness to give attention & 361\\ \hline
advantage, advancement or favor & 178\\ \hline
activity that one gives attention to & 66\\ \hline
causing attention to be given to & 11\\
\hline
{\bf Всего} & 2368\\
\hline
\end{tabular}
\label{tbl4_chu}
\end{table}


\parindent=0,5cm
\textbf{Результаты экспериментов.} Итогом проделанной работы стали обучение и проверка 81 наивного байесовского классификатора на многозначных словах \textit{line} и \textit{interest.} Точность разрешения лексической многозначности составила 89\% для слова \textit{interest} и 88\% для слова \textit{line.} В \cite{Pedersen 2000} было получено, что ансамбль классификаторов с голосованием простым большинством дает более высокую точность, чем взвешенное голосование. Например, для слова \textit{interest} при голосовании простым большинством точность составила 89\%, а взвешенное голосование дало только 83\%.
